---
title: "Lab 4: Introduction To Two-Predictor Regression"
author: "Fabio Setti"
institute: "PSYC 7804 - Regression with Lab"
bibliography: Additional files/R packages.bib
csl: Additional files/apa.csl
title-slide-attributes:
  data-transition: "zoom"
  data-visibility: "uncounted"
format:
   revealjs:
      footer: "Lab 4: Introduction To Two-Predictor Regression"
      width: 1280
      height: 720
      chalkboard: true
      slide-number: c/t 
      theme: Fabio_theme/Fabio_theme.scss
      navigation-mode: linear
      controls: false
      auto-stretch: false
      header-includes:
        - <script src="Fabio_theme/Fabio_theme.js"></script>

editor: source
---

## Today's Packages and Data 🤗

:::: {.columns}
::: {.column width="50%"}

```{r}
#| code-fold: true
#| eval: false
#| echo: true
#| code-line-numbers: false
#| code-summary: "Install Packages Code"
#| classes: code-150


install.packages("GGally")
install.packages("plotly")
# made some changes to the package, the updated version
devtools::install_github("quinix45/FabioFun")
# Should be installed already
# install.packages("rio")
# install.packages("tidyverse")

```

```{r}
#| eval: true
#| echo: true
#| code-line-numbers: false
#| warning: false
#| classes: code-150

library(GGally)
library(plotly)
library(tidyverse)
theme_set(theme_classic(base_size = 16, 
                        base_family = 'serif'))
```

</br>

<div style="font-size: 26px">

::: {.panel-tabset}

### `GGally`

The `GGally` package [@Schloerke_etal_2024] builds upon `ggplot2` and includes many fucntions for creating complex plots. 

### `plotly`

The `plotly` package [@Sievert_etal_2024a] is a Python and R package used to create interactive visualizations with JavaScript elements. 

:::
</div>

:::
::: {.column width="50%"}

<center> **Data** </center>

</br>

```{r}
#| warning: false
#| classes: code-125
#| echo: true
#| code-line-numbers: false
#| output: true

WH_2024 <- rio::import("https://github.com/quinix45/PSYC-7804-Regression-Lab-Slides/raw/refs/heads/main/Slides%20Files/Data/World_happiness_2024.csv")

# let's peak at our variables
str(WH_2024, vec.len = 2)
```

:::
::::


## Variables of Interest

We eventually want to look at how `Log_GDP` and `Freedom` impact `Happiness_score` across the countries in our data. Let's first explore the variables.


:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=1}

<ul style="font-size: 24px">  

<li>  The correlation matrix for the variables: </li>

</ul>


```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

# selecting just the variables of interest makes code cleaner later
reg_vars <- WH_2024[, c("Happiness_score",
                        "Log_GDP",
                        "Freedom")]

# Just the correlation table
cor(reg_vars)
```
:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=2}

<ul style="font-size: 24px">  

<li>  Some descriptive statistics: </li>

</ul>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

psych::describe(reg_vars)
```

:::

:::
::::

## Helpful Visualization

The `ggpairs()` function from the `GGally` package creates a visualizations that provides a lot of information in one go!

:::: {.columns}
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

ggpairs(reg_vars)
```


:::
::: {.column width="30%"}

<br>

<ul style="font-size: 26px">  

<li> **Upper triangle**: correlations among variables </li>

<li> **Long diagonal**: distribution of variables </li>

<li> **Lower diagonal**: scatterplots among variables </li>

</ul>




:::
::::

## Individual Regressions


:::: {.columns}
::: {.column width="50%"}

<ul style="font-size: 24px">  

<li>  `Log_GDP` only regression </li>

</ul>
::: {.fragment fragment-index=1}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

reg_GDP <- lm(Happiness_score ~ Log_GDP, 
              reg_vars)
summary(reg_GDP)
```
:::
:::
::: {.column width="50%"}

<ul style="font-size: 24px">  

<li>  `Freedom` only regression </li>

</ul>

::: {.fragment fragment-index=2}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

reg_free <- lm(Happiness_score ~ Freedom, 
              reg_vars)
summary(reg_free)
```
:::

:::
::::

## Regression With Both Predictors


:::: {.columns}
::: {.column width="60%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

reg_full <- lm(Happiness_score ~ Log_GDP + Freedom, 
          reg_vars)

summary(reg_full)
```

:::
::: {.column width="40%"}

As you can see, the regression coefficients of the two predictors change when both variables are included 🧐

<br>

Let's visualize what is happening!

:::
::::


## Individual Regression plots 

These are the equivalent plots to the individual regressions:

:::: {.columns}
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot Code"

  ggplot(reg_vars,  
 aes(x = Log_GDP, y = Happiness_score)) +
 geom_point() + 
    geom_smooth(method = "lm", 
                formula = "y~x", 
                se = FALSE)
```



:::
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot Code"

  ggplot(reg_vars,  
 aes(x = Freedom, y = Happiness_score)) +
 geom_point() + 
    geom_smooth(method = "lm", 
                formula = "y~x", 
                se = FALSE)
```

:::
::::


## Adding Slopes from multiple regression? 

Let's add the regression lines estimated from the regression with both predictors...

::: {.fragment fragment-index=1}

:::: {.columns}
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot Code"


  ggplot(reg_vars,  
 aes(x = Log_GDP, y = Happiness_score)) +
 geom_point() + 
    geom_smooth(method = "lm", 
                formula = "y~x", 
                se = FALSE) +
    geom_abline(intercept = coef(reg_full)[1], 
                slope = coef(reg_full)[2])
```


:::
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot Code"


  ggplot(reg_vars,  
 aes(x = Freedom, y = Happiness_score)) +
 geom_point() + 
    geom_smooth(method = "lm", 
                formula = "y~x", 
                se = FALSE) +
    geom_abline(intercept = coef(reg_full)[1], 
                slope = coef(reg_full)[3])
```
:::
::::

:::

::: {.fragment fragment-index=2}

Hold up, the lines from the multiple regression results seem way off !?

:::

::: {.fragment fragment-index=3}

💡 Maybe we need a change of perspective!

:::


## Looking Inside the Box

Now that we are dealing with 3 variables (`Log_GDP`, `Freedom`, `Happiness_score`), our data points are in a 3D box, not a 2D plot. 

:::: {.columns}
::: {.column width="40%"}

::: {.fragment fragment-index=1}

I made a function for making interactive 3D plots easily 

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125


# run "devtools::install_github("quinix45/FabioFun")" if you haven't installed the package yet

library(FabioFun)

nice_3D_plot(y = reg_vars$Happiness_score,
             x1 = reg_vars$Log_GDP,
             x2 = reg_vars$Freedom,
             dot_labels = WH_2024$Country_name,
             axis_names = c("Happiness", 
                            "GDP", 
                            "Freedom"))
```

:::


::: {.fragment fragment-index=3}
The 2D plots from the previous slide are just the *sides of the box* 🤓
:::


:::
::: {.column width="60%"}

::: {.fragment fragment-index=2}


```{r out.width = "100%"}

library(FabioFun)

nice_3D_plot(y = reg_vars$Happiness_score,
                            x1 = reg_vars$Log_GDP,
                            x2 = reg_vars$Freedom,
                            dot_labels = WH_2024$Country_name,
                              axis_names = c("Happiness", 
                                             "GDP", 
                                             "Freedom"),
                              plane_res = 20,
                              reg_plane = FALSE)%>%  
                    bslib::card(full_screen = TRUE)
```

:::
:::
::::

## The Regression Plane 

<div style="font-size: 24px">  

Remember that in linear regression with 1 predictor we are looking for the line that on average is closest to all point. When we have 2 predictors we are looking for the **plane** that is closest to all the points!

</div>


:::: {.columns}
::: {.column width="40%"}

::: {.fragment fragment-index=1}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = reg_vars$Happiness_score,
             x1 = reg_vars$Log_GDP,
             x2 = reg_vars$Freedom,
             dot_labels = WH_2024$Country_name,
             axis_names = c("Happiness", 
                            "GDP", 
                            "Freedom"),
             plane_res = 20,
             # add regression plane
             reg_plane = TRUE) 

```

:::

::: {.fragment fragment-index=2}

::: {.callout-note title="Question?"}
If this is the representation of regression with 2 predictors, what happens once we have 3 or more predictors? Can we visualize that? 
:::

:::

:::
::: {.column width="60%"}

::: {.fragment fragment-index=1}

```{r out.width = "100%"}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = reg_vars$Happiness_score,
             x1 = reg_vars$Log_GDP,
             x2 = reg_vars$Freedom,
             dot_labels = WH_2024$Country_name,
             axis_names = c("Happiness", 
                            "GDP", 
                            "Freedom"),
             plane_res = 20,
             reg_plane = TRUE) %>%  
  bslib::card(full_screen = TRUE)

```

:::

:::
::::


## Interpretation of Results

Now that we have a sense  of what we are looking at, let's interpret the output:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| 
summary(reg_full)
```
:::
::: {.column width="60%"}

<ul style="font-size: 24px">  

$\widehat{\mathrm{Happiness}} = 1.44 + 1.68\times \mathrm{Log\_GDP} + 2.86\times \mathrm{Freedom}$

<br>

::: {.fragment fragment-index=1}
<li> $b_0 = 1.44$: the expected `happpiness_score` when `GDP` and `Freedom` are at 0.  </li>
:::

::: {.fragment fragment-index=2}
<li> $b_1 = 1.68$: the expected increase in `happpiness_score` per each 1-unit increase in `Log_GDP` when accounting for `Freedom`.  </li>
:::

::: {.fragment fragment-index=3}
<li> $b_2 = 2.86$: the expected increase in `happpiness_score` per each 1-unit increase in `Freedom` when accounting for `GDP`.  </li>
:::

</ul>


::: {.fragment fragment-index=4}
$b_1$ and $b_2$ are *partial regression coefficients*. We will come back to this in [Lab 5](https://raw.githack.com/quinix45/PSYC-7804-Regression-Lab-Slides/main/Slides%20Files/Lab%205.html#/partial-regression-coefficients){target="_blank"}, so stay tuned.
:::

:::
::::


## Back to our friend *R* <sup>2</sup>

You can think of variance as how unpredictable something is. Higher variance means more unpredictability. At the same time, no variance means perfect predictability!


:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=1}

For example, if you generate data for a variable that has 0 variance, you always get the mean

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

rnorm(n = 5, 
      mean = 2, 
      sd = 0)
```
:::

::: {.fragment fragment-index=2}
Here, if we predict 2 we are always right, because the variable has no variance and is perfectly predictable. 
:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=3}

Let's return to our 1D scatterplot, and let's visualize the variance of the `Happiness_score` 

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true

ggplot(reg_vars,  
       aes(x = Happiness_score, y = 0)) +
       geom_point(shape = 1, 
                  size=6.5) +
      annotate("text", x = 4.7, y = .02, 
             label = "Variance of Happiness_score") +
xlim(1.5, 8.2) +
    ylim(-.1, .03) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y =  element_blank())
```
:::
:::
::::


## Variance of Residuals after regression {auto-animate="true"}

Although this may not sound very intuitive, you should think of residuals as a new version of your $Y$ variable, but with some unpredictability taken out thanks to our regression line. Less unpredictability means *lower variance*


:::: {.columns}
::: {.column width="30%"}

::: {.fragment fragment-index=1}

On the right, we have the residuals of `Happiness_score` after using `freedom` as predictor

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true

ggplot(reg_vars,  
       aes(x = Happiness_score, y = 0)) +
       geom_point(shape = 1, 
                  size=4.5) +
      annotate("text", x = 4.7, y = .02, 
             label = "Variance of Happiness_score") +
  geom_point(aes(x = residuals(reg_free) + 
                       # add mean to have residuals on same scale as 
                       # previous graph (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.05),
             shape = 1,
             size = 4.5, color = "blue") +
        annotate("text", x = 4.7, y = -0.03, 
             label = "Residual Variance of Happiness_score after using freedom as predictor", color = "blue") +
xlim(1.5, 8.2) +
    ylim(-.1, .03) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y =  element_blank())
```


Much less spread out, right?

:::
:::

::: {.column width="70%"}

::: {.fragment fragment-index=1}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

ggplot(reg_vars,  
       aes(x = Happiness_score, y = 0)) +
       geom_point(shape = 1, 
                  size=4.5) +
      annotate("text", x = 4.7, y = .02, 
             label = "Variance of Happiness_score") +
  geom_point(aes(x = residuals(reg_free) + 
                       # add mean to have residuals on same scale as 
                       # previous graph (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.05),
             shape = 1,
             size = 4.5, color = "blue") +
        annotate("text", x = 4.7, y = -0.03, 
             label = "Residual Variance of Happiness_score after using freedom as predictor", color = "blue") +
xlim(1.5, 8.2) +
    ylim(-.1, .03) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y =  element_blank())
```

:::
:::
::::


## Variance of Residuals after Regression {auto-animate="true" data-visibility="uncounted"}

Although this may not sound very intuitive, you should think of residuals as a new version of your $Y$ variable, but with some unpredictability taken out thanks to our regression line. Less unpredictability means *lower variance*

:::: {.columns}
::: {.column width="30%"}

Now, after we add both `freedom` and `GDP` as predictors, we se that the dots are even less spread out

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true

ggplot(reg_vars,  
       aes(x = Happiness_score, y = 0)) +
       geom_point(shape = 1, 
                  size=4.5)+
      annotate("text", x = 4.7, y = .02, 
             label = "Variance of Happiness_score") +
  geom_point(aes(x = residuals(reg_free) + 
                       # add mean to have residuals on same scale as 
                       # original variable (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.05),
             shape = 1,
             size = 4.5, color = "blue")  +
        annotate("text", x = 4.7, y = -0.03, 
             label = "Residual Variance of Happiness_score after using freedom as predictor", color = "blue") +
  geom_point(aes(x = residuals(reg_full) + 
                       # add mean to have residuals on same scale as 
                       # original variable (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.1),
             shape = 1,
             size = 4.5, color = "red") +
  annotate("text", x = 4.7, y = -0.08, 
             label = "Residual Variance of Happiness_score after using freedom  and GPD as predictors", color = "red") +
xlim(1.5, 8.2) +
    ylim(-.1, .03) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y =  element_blank())
```

The red dots are even less spread out than the blue ones!

:::
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125


ggplot(reg_vars,  
       aes(x = Happiness_score, y = 0)) +
       geom_point(shape = 1, 
                  size=4.5)+
      annotate("text", x = 4.7, y = .02, 
             label = "Variance of Happiness_score") +
  geom_point(aes(x = residuals(reg_free) + 
                       # add mean to have residuals on same scale as 
                       # original variable (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.05),
             shape = 1,
             size = 4.5, color = "blue")  +
        annotate("text", x = 4.7, y = -0.03, 
             label = "Residual Variance of Happiness_score after using freedom as predictor", color = "blue") +
  geom_point(aes(x = residuals(reg_full) + 
                       # add mean to have residuals on same scale as 
                       # original variable (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.1),
             shape = 1,
             size = 4.5, color = "red") +
  annotate("text", x = 4.7, y = -0.08, 
             label = "Residual Variance of Happiness_score after using freedom  and GPD as predictors", color = "red") +
xlim(1.5, 8.2) +
    ylim(-.1, .03) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y =  element_blank())
```

:::
::::

## Variance Explained

As we saw previously, the more variables we added, the less spread out our residuals were. A part of the variance of the original variable was *explained by our predictors*. 

:::: {.columns}
::: {.column width="50%"}

The variances of the three lines of dots were respectively:

<ol style="font-size: 22px">  

::: {.fragment fragment-index=1}

<li> Variance of `happiness_score`</li>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

var(reg_vars$Happiness_score)
```

:::

::: {.fragment fragment-index=2}

<li> Variance of `happiness_score` after using `freedom` as a predictor was </li>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
var(residuals(reg_free))
```

:::

::: {.fragment fragment-index=3}

<li> Variance of `happiness_score` after using both `freedom` and `GDP` as a predictors was </li>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

var(residuals(reg_full))
```

:::

</ol>
:::


::: {.column width="50%"}


<center>

::: {.fragment fragment-index=4}

We can easily calculate $R^2$ by hand

:::

<ul style="font-size: 22px">  


::: {.fragment fragment-index=4}

<li> $R^2$ after using `freedom` as a predictor </li>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-150

1 - (0.8158336/1.395344)
```

:::

::: {.fragment fragment-index=5}

<li> $R^2$ after using `freedom` and `GDP` as predictors  </li>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

1 - (0.3925608/1.395344)
```

:::

::: {.fragment fragment-index=6}

and we can check that our hand calculation match the regression output

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

summary(reg_free)$r.squared
summary(reg_full)$r.squared
```

:::

</ul>
</center>
:::
::::


## References 

<div id="refs"> </div>


# Appendix: `nice_3D_plot()` Showcase


## Example with World Happiness Data

You can also add groups to the dots of the 3D plot. Here I am using `Region`.

:::: {.columns}
::: {.column width="50%"}

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = reg_vars$Happiness_score,
             x1 = reg_vars$Log_GDP,
             x2 = reg_vars$Freedom,
             dot_labels = WH_2024$Country_name,
             # add groups
             groups = WH_2024$Region,
             axis_names = c("Happiness", 
                            "GDP", 
                            "Freedom"),
             plane_res = 20,
             reg_plane = FALSE)

```


There are a bit too many regions, so this is not very practical in this case, but this just an example.

:::


::: {.column width="50%"}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = reg_vars$Happiness_score,
                            x1 = reg_vars$Log_GDP,
                            x2 = reg_vars$Freedom,
                            dot_labels = WH_2024$Country_name,
                            groups = WH_2024$Region,
                              axis_names = c("Happiness", 
                                             "GDP", 
                                             "Freedom"),
                              plane_res = 20,
                              reg_plane = FALSE) %>%  
                    bslib::card(full_screen = TRUE)

```

:::
::::

## Example with iris Data

A more practical example with the [iris](https://en.wikipedia.org/wiki/Iris_flower_data_set){target="_blank"} dataset, Let's start with just providing 3 variables.

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width)
```



:::
::: {.column width="60%"}


```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width) %>%  
  bslib::card(full_screen = TRUE)
```

:::
::::


## Name Axes

Let's name our axes:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"))
```



:::
::: {.column width="60%"}


```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW")) %>%  
  bslib::card(full_screen = TRUE)
```

:::
::::




## Add Groups 

Let's add colors for the 3 different species of flowers:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species)
```


:::
::: {.column width="60%"}


```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             groups = iris$Species,
             axis_names = c("SL", "PL", "PW")) %>%  
  bslib::card(full_screen = TRUE)
```

:::
::::

## Add Regression Plane

Let's add a regression plane:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species,
             reg_plane = TRUE)
```


:::
::: {.column width="60%"}


```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             groups = iris$Species,
             axis_names = c("SL", "PL", "PW"),
             reg_plane = TRUE)%>%  
  bslib::card(full_screen = TRUE)
```

:::
::::


## Pass Arguments to `plot_Ly()`

You can also pass arguments to the `plot_ly()` function. Let's make the dots more transparent by passing the `opacity =` argument to `plot_ly()`:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species,
             reg_plane = TRUE,
             opacity = .3)
```


See [here](https://www.r-bloggers.com/2015/02/r-three-dots-ellipsis/){target="_blank"} for what "passing arguments" to a function within a function means in R. 


:::
::: {.column width="60%"}


```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125


nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species,
             reg_plane = TRUE,
             opacity = .3)%>%  
  bslib::card(full_screen = TRUE)
```

:::
::::


## Add Interaction Term to Regression Plane

You can also specify an interaction between the two predictors and get an interaction plane. You will see that the reression plane now bends:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species,
             reg_plane = TRUE,
             opacity = .3,
             interaction = TRUE)
```

See [Lab 9](https://raw.githack.com/quinix45/PSYC-7804-Regression-Lab-Slides/main/Slides%20Files/Lab%209.html#/title-slide){target="_blank"} if you are curious to know what is happening. 


:::
::: {.column width="60%"}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125


nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species,
             reg_plane = TRUE,
             opacity = .3,
             interaction = TRUE)%>%  
  bslib::card(full_screen = TRUE)
```

:::
::::



