---
title: "Lab 4: Introduction To Two-Predictor Regression"
author: "Fabio Setti"
institute: "PSYC 7804 - Regression with Lab"
bibliography: Additional files/R packages.bib
csl: Additional files/apa.csl
title-slide-attributes:
  data-transition: "zoom"
  data-visibility: "uncounted"
format:
   revealjs:
      footer: "Lab 4: Introduction To Two-Predictor Regression"
      width: 1280
      height: 720
      chalkboard: true
      slide-number: c/t 
      theme: Fabio_theme/Fabio_theme.scss
      navigation-mode: linear
      controls: false
      auto-stretch: false
      header-includes:
        - <script src="Fabio_theme/Fabio_theme.js"></script>

editor: source
---

## Font-awesome slide{visibility="hidden"}

uncounted and not visible slide to trigger the fontawesome extension

{{< fa thumbs-up >}} 


## Today's Packages and Data 🤗

:::: {.columns}
::: {.column width="50%"}

```{r}
#| code-fold: true
#| eval: false
#| echo: true
#| code-line-numbers: false
#| code-summary: "Install Packages Code"
#| classes: code-150


install.packages("GGally")
install.packages("plotly")
# made some changes to the package, the updated version
devtools::install_github("quinix45/FabioFun")
# Should be installed already
# install.packages("rio")
# install.packages("tidyverse")

```

```{r}
#| eval: true
#| echo: true
#| code-line-numbers: false
#| warning: false
#| classes: code-150

library(GGally)
library(plotly)
library(tidyverse)
theme_set(theme_classic(base_size = 16, 
                        base_family = 'serif'))
```

</br>

<div style="font-size: 26px">

::: {.panel-tabset}

### `GGally`

The `GGally` package [@Schloerke_etal_2024] builds upon `ggplot2` and includes many fucntions for creating complex plots. 

### `plotly`

The `plotly` package [@Sievert_etal_2024a] is a Python and R package used to create interactive visualizations with JavaScript elements. 

:::
</div>

:::
::: {.column width="50%"}

<center> **Data** </center>

</br>

```{r}
#| warning: false
#| classes: code-125
#| echo: true
#| code-line-numbers: false
#| output: true

WH_2024 <- rio::import("https://github.com/quinix45/PSYC-7804-Regression-Lab-Slides/raw/refs/heads/main/Slides%20Files/Data/World_happiness_2024.csv")

# let's peak at our variables
str(WH_2024, vec.len = 2)
```

:::
::::


## Variables of Interest

We eventually want to look at how `Log_GDP` and `Freedom` impact `Happiness_score` across the countries in our data. Let's first explore the variables.


:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=1}

<ul style="font-size: 24px">  

<li>  The correlation matrix for the variables: </li>

</ul>


```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

# selecting just the variables of interest makes code cleaner later
reg_vars <- WH_2024[, c("Happiness_score",
                        "Log_GDP",
                        "Freedom")]

# Just the correlation table
cor(reg_vars)
```
:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=2}

<ul style="font-size: 24px">  

<li>  Some descriptive statistics: </li>

</ul>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

psych::describe(reg_vars)
```

:::

:::
::::

## Helpful Visualization

The `ggpairs()` function from the `GGally` package creates a visualizations that provides a lot of information in one go!

:::: {.columns}
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

ggpairs(reg_vars)
```


:::
::: {.column width="30%"}

<br>

<ul style="font-size: 26px">  

<li> **Upper triangle**: correlations among variables </li>

<li> **Long diagonal**: distribution of variables </li>

<li> **Lower diagonal**: scatterplots among variables </li>

</ul>




:::
::::

## Individual Regressions


:::: {.columns}
::: {.column width="50%"}

<ul style="font-size: 24px">  

<li>  `Log_GDP` only regression </li>

</ul>
::: {.fragment fragment-index=1}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

reg_GDP <- lm(Happiness_score ~ Log_GDP, 
              reg_vars)
summary(reg_GDP)
```
:::
:::
::: {.column width="50%"}

<ul style="font-size: 24px">  

<li>  `Freedom` only regression </li>

</ul>

::: {.fragment fragment-index=2}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

reg_free <- lm(Happiness_score ~ Freedom, 
              reg_vars)
summary(reg_free)
```
:::

:::
::::

## Regression With Both Predictors


:::: {.columns}
::: {.column width="60%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

reg_full <- lm(Happiness_score ~ Log_GDP + Freedom, 
          reg_vars)

summary(reg_full)
```

:::
::: {.column width="40%"}

As you can see, the regression coefficients of the two predictors change when both variables are included 🧐

<br>

Let's visualize what is happening!

:::
::::


## Individual Regression plots 

These are the equivalent plots to the individual regressions:

:::: {.columns}
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot Code"

  ggplot(reg_vars,  
 aes(x = Log_GDP, y = Happiness_score)) +
 geom_point() + 
    geom_smooth(method = "lm", 
                formula = "y~x", 
                se = FALSE)
```



:::
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot Code"

  ggplot(reg_vars,  
 aes(x = Freedom, y = Happiness_score)) +
 geom_point() + 
    geom_smooth(method = "lm", 
                formula = "y~x", 
                se = FALSE)
```

:::
::::


## Adding Slopes from multiple regression? 

Let's add the regression lines estimated from the regression with both predictors...

::: {.fragment fragment-index=1}

:::: {.columns}
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot Code"


  ggplot(reg_vars,  
 aes(x = Log_GDP, y = Happiness_score)) +
 geom_point() + 
    geom_smooth(method = "lm", 
                formula = "y~x", 
                se = FALSE) +
    geom_abline(intercept = coef(reg_full)[1], 
                slope = coef(reg_full)[2])
```


:::
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot Code"


  ggplot(reg_vars,  
 aes(x = Freedom, y = Happiness_score)) +
 geom_point() + 
    geom_smooth(method = "lm", 
                formula = "y~x", 
                se = FALSE) +
    geom_abline(intercept = coef(reg_full)[1], 
                slope = coef(reg_full)[3])
```
:::
::::

:::

::: {.fragment fragment-index=2}

Hold up, the lines from the multiple regression results seem way off !?

:::

::: {.fragment fragment-index=3}

💡 Maybe we need a change of perspective!

:::


## Looking Inside the Box

Now that we are dealing with 3 variables (`Log_GDP`, `Freedom`, `Happiness_score`), our data points are in a 3D box, not a 2D plot. 

:::: {.columns}
::: {.column width="40%"}

::: {.fragment fragment-index=1}

I made a function for making interactive 3D plots easily 

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125


# run "devtools::install_github("quinix45/FabioFun")" if you haven't installed the package yet

library(FabioFun)

nice_3D_plot(y = reg_vars$Happiness_score,
             x1 = reg_vars$Log_GDP,
             x2 = reg_vars$Freedom,
             dot_labels = WH_2024$Country_name,
             axis_names = c("Happiness", 
                            "GDP", 
                            "Freedom"))
```

:::


::: {.fragment fragment-index=3}
The 2D plots from the previous slide are just the *sides of the box* 🤓
:::


:::
::: {.column width="60%"}

::: {.fragment fragment-index=2}


```{r out.width = "100%"}

library(FabioFun)

nice_3D_plot(y = reg_vars$Happiness_score,
                            x1 = reg_vars$Log_GDP,
                            x2 = reg_vars$Freedom,
                            dot_labels = WH_2024$Country_name,
                              axis_names = c("Happiness", 
                                             "GDP", 
                                             "Freedom"),
                              plane_res = 20,
                              reg_plane = FALSE)%>%  
                    bslib::card(full_screen = TRUE)
```

:::
:::
::::

## The Regression Plane 

<div style="font-size: 24px">  

Remember that in linear regression with 1 predictor we are looking for the line that on average is closest to all point. When we have 2 predictors we are looking for the **plane** that is closest to all the points!

</div>


:::: {.columns}
::: {.column width="40%"}

::: {.fragment fragment-index=1}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = reg_vars$Happiness_score,
             x1 = reg_vars$Log_GDP,
             x2 = reg_vars$Freedom,
             dot_labels = WH_2024$Country_name,
             axis_names = c("Happiness", 
                            "GDP", 
                            "Freedom"),
             plane_res = 20,
             # add regression plane
             reg_plane = TRUE) 

```

:::

::: {.fragment fragment-index=2}

::: {.callout-note title="Question?"}
If this is the representation of regression with 2 predictors, what happens once we have 3 or more predictors? Can we visualize that? 
:::

:::

:::
::: {.column width="60%"}

::: {.fragment fragment-index=1}

```{r out.width = "100%"}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = reg_vars$Happiness_score,
             x1 = reg_vars$Log_GDP,
             x2 = reg_vars$Freedom,
             dot_labels = WH_2024$Country_name,
             axis_names = c("Happiness", 
                            "GDP", 
                            "Freedom"),
             plane_res = 20,
             reg_plane = TRUE) %>%  
  bslib::card(full_screen = TRUE)

```

:::

:::
::::


## Interpretation of Results

Now that we have a sense  of what we are looking at, let's interpret the output:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| 
summary(reg_full)
```
:::
::: {.column width="60%"}

<ul style="font-size: 24px">  

$\widehat{\mathrm{Happiness}} = 1.44 + 1.68\times \mathrm{Log\_GDP} + 2.86\times \mathrm{Freedom}$

<br>

::: {.fragment fragment-index=1}
<li> $b_0 = 1.44$: the expected `happpiness_score` when `GDP` and `Freedom` are at 0.  </li>
:::

::: {.fragment fragment-index=2}
<li> $b_1 = 1.68$: the expected increase in `happpiness_score` per each 1-unit increase in `Log_GDP` when accounting for `Freedom`.  </li>
:::

::: {.fragment fragment-index=3}
<li> $b_2 = 2.86$: the expected increase in `happpiness_score` per each 1-unit increase in `Freedom` when accounting for `GDP`.  </li>
:::

</ul>


::: {.fragment fragment-index=4}
$b_1$ and $b_2$ are *partial regression coefficients*. We will come back to this in [Lab 5](https://raw.githack.com/quinix45/PSYC-7804-Regression-Lab-Slides/main/Slides%20Files/Lab%205.html#/partial-regression-coefficients){target="_blank"}, so stay tuned.
:::

:::
::::


## Back to our friend *R* <sup>2</sup>

You can think of variance as how unpredictable something is. Higher variance means more unpredictability. At the same time, no variance means perfect predictability!


:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=1}

For example, if you generate data for a variable that has 0 variance, you always get the mean

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

rnorm(n = 5, 
      mean = 2, 
      sd = 0)
```
:::

::: {.fragment fragment-index=2}
Here, if we predict 2 we are always right, because the variable has no variance and is perfectly predictable. 
:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=3}

Let's return to our 1D scatterplot, and let's visualize the variance of the `Happiness_score` 

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true

ggplot(reg_vars,  
       aes(x = Happiness_score, y = 0)) +
       geom_point(shape = 1, 
                  size=6.5) +
      annotate("text", x = 4.7, y = .02, 
             label = "Variance of Happiness_score") +
xlim(1.5, 8.2) +
    ylim(-.1, .03) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y =  element_blank())
```
:::
:::
::::


## Variance of Residuals after regression {auto-animate="true"}

Although this may not sound very intuitive, you should think of residuals as a new version of your $Y$ variable, but with some unpredictability taken out thanks to our regression line. Less unpredictability means *lower variance*


:::: {.columns}
::: {.column width="30%"}

::: {.fragment fragment-index=1}

On the right, we have the residuals of `Happiness_score` after using `freedom` as predictor

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true

ggplot(reg_vars,  
       aes(x = Happiness_score, y = 0)) +
       geom_point(shape = 1, 
                  size=4.5) +
      annotate("text", x = 4.7, y = .02, 
             label = "Variance of Happiness_score") +
  geom_point(aes(x = residuals(reg_free) + 
                       # add mean to have residuals on same scale as 
                       # previous graph (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.05),
             shape = 1,
             size = 4.5, color = "blue") +
        annotate("text", x = 4.7, y = -0.03, 
             label = "Residual Variance of Happiness_score after using freedom as predictor", color = "blue") +
xlim(1.5, 8.2) +
    ylim(-.1, .03) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y =  element_blank())
```


Much less spread out, right?

:::
:::

::: {.column width="70%"}

::: {.fragment fragment-index=1}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

ggplot(reg_vars,  
       aes(x = Happiness_score, y = 0)) +
       geom_point(shape = 1, 
                  size=4.5) +
      annotate("text", x = 4.7, y = .02, 
             label = "Variance of Happiness_score") +
  geom_point(aes(x = residuals(reg_free) + 
                       # add mean to have residuals on same scale as 
                       # previous graph (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.05),
             shape = 1,
             size = 4.5, color = "blue") +
        annotate("text", x = 4.7, y = -0.03, 
             label = "Residual Variance of Happiness_score after using freedom as predictor", color = "blue") +
xlim(1.5, 8.2) +
    ylim(-.1, .03) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y =  element_blank())
```

:::
:::
::::


## Variance of Residuals after Regression {auto-animate="true" data-visibility="uncounted"}

Although this may not sound very intuitive, you should think of residuals as a new version of your $Y$ variable, but with some unpredictability taken out thanks to our regression line. Less unpredictability means *lower variance*

:::: {.columns}
::: {.column width="30%"}

Now, after we add both `freedom` and `GDP` as predictors, we see that the dots are even less spread out

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true

ggplot(reg_vars,  
       aes(x = Happiness_score, y = 0)) +
       geom_point(shape = 1, 
                  size=4.5)+
      annotate("text", x = 4.7, y = .02, 
             label = "Variance of Happiness_score") +
  geom_point(aes(x = residuals(reg_free) + 
                       # add mean to have residuals on same scale as 
                       # original variable (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.05),
             shape = 1,
             size = 4.5, color = "blue")  +
        annotate("text", x = 4.7, y = -0.03, 
             label = "Residual Variance of Happiness_score after using freedom as predictor", color = "blue") +
  geom_point(aes(x = residuals(reg_full) + 
                       # add mean to have residuals on same scale as 
                       # original variable (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.1),
             shape = 1,
             size = 4.5, color = "red") +
  annotate("text", x = 4.7, y = -0.08, 
             label = "Residual Variance of Happiness_score after using freedom  and GPD as predictors", color = "red") +
xlim(1.5, 8.2) +
    ylim(-.1, .03) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y =  element_blank())
```

The red dots are even less spread out than the blue ones!

:::
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125


ggplot(reg_vars,  
       aes(x = Happiness_score, y = 0)) +
       geom_point(shape = 1, 
                  size=4.5)+
      annotate("text", x = 4.7, y = .02, 
             label = "Variance of Happiness_score") +
  geom_point(aes(x = residuals(reg_free) + 
                       # add mean to have residuals on same scale as 
                       # original variable (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.05),
             shape = 1,
             size = 4.5, color = "blue")  +
        annotate("text", x = 4.7, y = -0.03, 
             label = "Residual Variance of Happiness_score after using freedom as predictor", color = "blue") +
  geom_point(aes(x = residuals(reg_full) + 
                       # add mean to have residuals on same scale as 
                       # original variable (variance remains the same)
                         mean(reg_vars$Happiness_score), 
                     y = -.1),
             shape = 1,
             size = 4.5, color = "red") +
  annotate("text", x = 4.7, y = -0.08, 
             label = "Residual Variance of Happiness_score after using freedom  and GPD as predictors", color = "red") +
xlim(1.5, 8.2) +
    ylim(-.1, .03) +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y =  element_blank())
```

:::
::::

## Variance Explained

As we saw previously, the more variables we added, the less spread out our residuals were. A part of the variance of the original variable was *explained by our predictors*. 

:::: {.columns}
::: {.column width="50%"}

The variances of the three lines of dots were respectively:

<ol style="font-size: 22px">  

::: {.fragment fragment-index=1}

<li> Variance of `happiness_score`</li>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

var(reg_vars$Happiness_score)
```

:::

::: {.fragment fragment-index=2}

<li> Variance of `happiness_score` after using `freedom` as a predictor was </li>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
var(residuals(reg_free))
```

:::

::: {.fragment fragment-index=3}

<li> Variance of `happiness_score` after using both `freedom` and `GDP` as a predictors was </li>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

var(residuals(reg_full))
```

:::

</ol>
:::


::: {.column width="50%"}


<center>

::: {.fragment fragment-index=4}

We can easily calculate $R^2$ by hand

:::

<ul style="font-size: 22px">  


::: {.fragment fragment-index=4}

<li> $R^2$ after using `freedom` as a predictor </li>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-150

1 - (0.8158336/1.395344)
```

:::

::: {.fragment fragment-index=5}

<li> $R^2$ after using `freedom` and `GDP` as predictors  </li>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

1 - (0.3925608/1.395344)
```

:::

::: {.fragment fragment-index=6}

and we can check that our hand calculation match the regression output

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

summary(reg_free)$r.squared
summary(reg_full)$r.squared
```

:::

</ul>
</center>
:::
::::

## Significance For *R* <sup>2</sup>

As mentioned in [Lab 3](https://quinix45.github.io/PSYC-7804-Regression-Lab-Slides/Slides%20Files/Lab%203.html#/sampling-distributions){target="_blank"}, significance testing targets the sampling distribution of a statistic. For regression coefficients, the sampling distribution is a $t$-distribution. For $R^2$, we use an $F-$distribution.


::: {.fragment fragment-index=1}
Now, why we use an $F-$distribution takes a few steps to explain, so I will go on a bit of a tangent. 
:::

<ol style="font-size: 22px">  

::: {.fragment fragment-index=2}
<li> As we just saw, $R^2$ tells you *the percentage of variance* (i.e., unpredictability) that was taken out of $Y$ by using all the predictors in a regression. How do we tests that the $R^2$ is not $0$ (null hypothesis)?  </li>
:::

::: {.fragment fragment-index=3}
<li> Some smart people have figured out that we can do a "transformation" of $R^2$, 

$$F = \frac{\mathrm{df_{residual}}\times R^2}{k(1 - R^2)},$$
Where $k$ is the number of predictors, and if $N$ is the sample size, $\mathrm{df_{residual}} = N - k - 1$. </li>
:::


::: {.fragment fragment-index=4}
<li> And it also turns out that if $R^2$ is truly $0$ and $H_0$ is true, we would expect to see $F = \frac{\mathrm{df_{residual}}}{\mathrm{df_{residual}} - 2}$. this is usually around $F = 1$. </li>
:::

</ul>

::: {.fragment fragment-index=5}
<div style="font-size: 24px; padding-top: 18px; text-align: center;"> Look at the formula for $F$ and plug in some $R^2$ values, you will see that $F$ increases as $R^2$ increases. And by the way, what happens to $F$ if $R^2 = 1$? </div>
:::

## Why $F$ and not $R^2$ Directly?

Why do we got thorough all this trouble to calculate $F$ and confuse students taking regression courses? 

::: {.fragment fragment-index=1}
The formula for $F$ on the previous slide accounts for a very important fact that we will return to in [Lab 6](https://quinix45.github.io/PSYC-7804-Regression-Lab-Slides/Slides%20Files/Lab%206.html#/the-catch-with-r2){target="_blank"}: $R^2$ always increases when you add additional predictors, even if the new predictors are completely unrelated to $Y$. 
:::

::: {.fragment fragment-index=2}
<div style="font-size: 26px; text-align: center;"> The denominator of $F$, $k(1 - R^2)$ accounts for that through the $k$ part. The raw $R^2$ does not. </div>
:::

::: {.fragment fragment-index=3}
The *sampling distribution* for an $F$-statistic calculated when $H_0$ is true and $R^2 = 0$ is, as the name suggests, an $F$-distribution with $\mathrm{df}_1 = k$, and $\mathrm{df}_2 = \mathrm{df_{residual}}$. 
:::

::: {.fragment fragment-index=4}
For significance testing, the further an $F$-value is from 1, the smaller the $p$-value will be.

::: {.callout-note title="ANOVA and all the $F$s"}
If you remember looking at ANOVA results, you should remember seeing a whole bunch of $F$-statistics. As we will see, ANOVA is a regression, but it focuses on variance explained ($R^2$!) by its predictors rather than regression coefficients. That is why all you will see from ANOVA are $F$-statistics.
:::
:::



## *P*-value from the $F$-distribution


We want to calculate how likely it is to get our $F$-value from an $F$-distribution with $\mathrm{df}_1 = k = 2$, and $\mathrm{df}_2 = \mathrm{df_{residual}} = 137$.

:::: {.columns}
::: {.column width="40%"}

::: {.fragment fragment-index=1}
If we look at our full regression, our $F$-value was $\frac{\mathrm{df_{residual}}\times R^2}{k(1 - R^2)} = \frac{137\times .63}{2(1 - .63)} \approx 175$
:::


::: {.fragment fragment-index=2}
<div style="font-size: 24px"> You can see that $175$ is way off to the right. The probability of getting $175$ or more from this distribution is the $p$-value: </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

# the probability for the F-distribution
pf(175, df1 = 2, df2 = 137, lower.tail = FALSE)
```
:::


::: {.fragment fragment-index=3}
<div style="font-size: 22px"> So, extremely unlikely ($p = 1.86\times10^{-38}$) to see $F = 175$ from the distribution on the right if $R^2 = 0$. We reject $H_0$. </div>
:::

:::
::: {.column width=60%"}


::: {.fragment fragment-index=2}
```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot Code"

ggplot() +
  geom_function(
    fun = df, 
    args = list(df1 = 2, df2 = 137), 
    col = "blue") + 
  labs(x = "All Possible F-values if R\U00B2 = 0", 
       y = "Density") +
  xlim(0, 10)

```
:::

:::
::::


## Looking at the regression Ouput


If we look at the regression output again, we will see that  $\mathrm{df}_1 = 2$, $\mathrm{df_{residual}} = 137$, $F = 175$, and $p = 1.86\times10^{-38}$ are in the line below the $R^2$ value.

:::: {.columns}
::: {.column width="70%"}


```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

summary(reg_full)
```


:::

::: {.column width="30%"}

::: {.fragment fragment-index=1}

In APA style you would report this at the end of the regression results as:

> ...the model explained a significant porportion of variance in country happiness, $R^2 = .72$, $F$(2, 137) = 175, $p < .001$. 

:::
::::
:::


## What does the $F$-distribution look like?

:::: {.columns}
::: {.column width="30%"}

<div style="font-size: 22px"> Where degrees of freedom come from is a very complex question (see [here](https://stats.stackexchange.com/questions/16921/how-to-understand-degrees-of-freedom){target="_blank"}). The practical effect that they have in significance testing is to *tell the sampling distribution to adapt to sample size and/or number of predictors*. </div>


::: {.fragment fragment-index=1}
We can see what the $F$-distribution looks like when we change degrees of freedom
:::


::: {.fragment fragment-index=2}
$\mathrm{df}_1$, representing number of variables, is much more influential on the shape compared to $\mathrm{df}_2$, which represents the sample size. 


<div style="font-size: 18px"> Note that $F$-values are *always positive*. </div>
:::

:::
::: {.column width="70%"}

<iframe width="90%" height="550px" src="https://fabiosetti.shinyapps.io/F_distribution/"> </iframe>
:::
::::


## Simulating Experiments where $H_0$ is true

<div style="font-size: 24px"> The idea of sampling distributions can be a bit abstract. To convince myself that smart math people are not lying to me, I sometime simulate things; simulating a large (not quite infinite) number of experiments if pretty straightforward in R! </div>


:::: {.columns}
::: {.column width="40%"}

::: {.fragment fragment-index=1}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

sample_size <- 300

# object where we save F-statistic
F_statistics <- c()

for(i in 1:2000){
  
X <- rnorm(sample_size, mean = 0, sd = 1)
# the regression coefficient of X is exactly 0
Y <- rnorm(sample_size, mean = 0*X, sd = 1)

reg_YX <- lm(Y ~ X)

# store F-statistic at every iteration
F_statistics[i] <-  as.numeric(summary(reg_YX)$fstatistic[1])
}

ggplot() +
  geom_density(aes(x = F_statistics)) +
      scale_y_continuous(expand = c(0, 0))
```
:::

:::
::: {.column width="60%"}


::: {.fragment fragment-index=1}

```{r}
#| eval: true
#| echo: false 
#| code-line-numbers: false
#| classes: code-125

sample_size <- 300

F_statistics <- c()

for(i in 1:2000){
  
X <- rnorm(sample_size, mean = 0, sd = 1)
# the regression coefficient of X is exactly 0
Y <- rnorm(sample_size, mean = 0*X, sd = 1)

reg_YX <- lm(Y ~ X)

# store F-statistic at every iteration
F_statistics[i] <-  as.numeric(summary(reg_YX)$fstatistic[1])
}

ggplot() +
  geom_density(aes(x = F_statistics)) +
      scale_y_continuous(expand = c(0, 0))
```

<div style="font-size: 20px"> On the left, there is code that simulates 2000 "experiments" with one predictor and $N = 300$ where $R^2 = 0$. If we save the $F$-values from each experiment and plot them, you will see that the plot matches an $F$-distribution with $\mathrm{df}_1 = 1$ and $\mathrm{df}_2 = 298$. Check with the plot on the previous slide! (make sure the range of the *x*-axis match) </div>
:::

:::
::::


## References 

<div id="refs"> </div>











# Appendix: `nice_3D_plot()` Showcase


## Example with World Happiness Data

You can also add groups to the dots of the 3D plot. Here I am using `Region`.

:::: {.columns}
::: {.column width="50%"}

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = reg_vars$Happiness_score,
             x1 = reg_vars$Log_GDP,
             x2 = reg_vars$Freedom,
             dot_labels = WH_2024$Country_name,
             # add groups
             groups = WH_2024$Region,
             axis_names = c("Happiness", 
                            "GDP", 
                            "Freedom"),
             plane_res = 20,
             reg_plane = FALSE)

```


There are a bit too many regions, so this is not very practical in this case, but this just an example.

:::


::: {.column width="50%"}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = reg_vars$Happiness_score,
                            x1 = reg_vars$Log_GDP,
                            x2 = reg_vars$Freedom,
                            dot_labels = WH_2024$Country_name,
                            groups = WH_2024$Region,
                              axis_names = c("Happiness", 
                                             "GDP", 
                                             "Freedom"),
                              plane_res = 20,
                              reg_plane = FALSE) %>%  
                    bslib::card(full_screen = TRUE)

```

:::
::::

## Example with iris Data

A more practical example with the [iris](https://en.wikipedia.org/wiki/Iris_flower_data_set){target="_blank"} dataset, Let's start with just providing 3 variables.

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width)
```



:::
::: {.column width="60%"}


```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width) %>%  
  bslib::card(full_screen = TRUE)
```

:::
::::


## Name Axes

Let's name our axes:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"))
```



:::
::: {.column width="60%"}


```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW")) %>%  
  bslib::card(full_screen = TRUE)
```

:::
::::




## Add Groups 

Let's add colors for the 3 different species of flowers:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species)
```


:::
::: {.column width="60%"}


```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             groups = iris$Species,
             axis_names = c("SL", "PL", "PW")) %>%  
  bslib::card(full_screen = TRUE)
```

:::
::::

## Add Regression Plane

Let's add a regression plane:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species,
             reg_plane = TRUE)
```


:::
::: {.column width="60%"}


```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             groups = iris$Species,
             axis_names = c("SL", "PL", "PW"),
             reg_plane = TRUE)%>%  
  bslib::card(full_screen = TRUE)
```

:::
::::


## Pass Arguments to `plot_ly()`

You can also pass arguments to the `plot_ly()` function. Let's make the dots more transparent by passing the `opacity =` argument to `plot_ly()`:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species,
             reg_plane = TRUE,
             opacity = .3)
```


See [here](https://www.r-bloggers.com/2015/02/r-three-dots-ellipsis/){target="_blank"} for what "passing arguments" to a function within a function means in R. 


:::
::: {.column width="60%"}


```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125


nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species,
             reg_plane = TRUE,
             opacity = .3)%>%  
  bslib::card(full_screen = TRUE)
```

:::
::::


## Add Interaction Term to Regression Plane

You can also specify an interaction between the two predictors and get an interaction plane. You will see that the reression plane now bends:

:::: {.columns}
::: {.column width="40%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species,
             reg_plane = TRUE,
             opacity = .3,
             interaction = TRUE)
```

See [Lab 9](https://raw.githack.com/quinix45/PSYC-7804-Regression-Lab-Slides/main/Slides%20Files/Lab%209.html#/title-slide){target="_blank"} if you are curious to know what is happening. 


:::
::: {.column width="60%"}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false
#| classes: code-125


nice_3D_plot(y = iris$Sepal.Length,
             x1 = iris$Petal.Length,
             x2 = iris$Petal.Width,
             axis_names = c("SL", "PL", "PW"),
             groups = iris$Species,
             reg_plane = TRUE,
             opacity = .3,
             interaction = TRUE)%>%  
  bslib::card(full_screen = TRUE)
```

:::
::::



