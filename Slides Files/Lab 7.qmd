---
title: "Lab 7: Multicollinearity and Dominance Analysis"
author: "Fabio Setti"
institute: "PSYC 7804 - Regression with Lab"
bibliography: Additional files/R packages.bib
csl: Additional files/apa.csl
notice: |
  @Fox_etal_2024
  @Wickham_RStudio_2023
title-slide-attributes:
  data-transition: "zoom"
  data-visibility: "uncounted"
format:
   revealjs:
      footer: "Lab 7: Multicollinearity and Dominance Analysis"
      width: 1280
      height: 720
      chalkboard: true
      slide-number: c/t 
      theme: Fabio_theme/Fabio_theme.scss
      navigation-mode: linear
      controls: false
      auto-stretch: false
      header-includes:
        - <script src="Fabio_theme/Fabio_theme.js"></script>

editor: source
---


## Today's Packages and Data ðŸ¤—

:::: {.columns}
::: {.column width="50%"}

```{r}
#| code-fold: true
#| eval: false
#| echo: true
#| code-line-numbers: false
#| code-summary: "Install Packages Code"
#| classes: code-150

install.packages("dominanceanalysis")
# install.packages("tidyverse")

```

```{r}
#| eval: true
#| echo: true
#| code-line-numbers: false
#| warning: false
#| classes: code-150

library(dominanceanalysis)
library(tidyverse)
theme_set(theme_classic(base_size = 16, 
                        base_family = 'serif'))
```

</br>

<div style="font-size: 26px">

::: {.panel-tabset}

### `dominanceanalysis`

The `dominanceanalysis` package [@Navarrete_etal_2024] contains functions to run dominance analysis in R.
:::

</div>

:::
::: {.column width="50%"}

<center> **Data** </center>

</br>

```{r}
#| warning: false
#| classes: code-125
#| echo: true
#| code-line-numbers: false
#| output: true

music <- rio::import("https://github.com/quinix45/PSYC-7804-Regression-Lab-Slides/raw/refs/heads/main/Slides%20Files/Data/music.csv") %>% 
  na.omit()

str(music, 
    give.attr = FALSE)
```

:::
::::

## Background About the data

I am using some of the data from @Setti_Kahn_2024, where<span style="font-size: 12px"> (yes I am citing myself, but it makes sense for this lab, I promise ðŸ˜¶ðŸ«£) </span> we looked at how facets of openness to experience (both from the  [big five](https://ipip.ori.org/newNEO_FacetsTable.htm){target="_blank"} and the [HEXACO](https://ipip.ori.org/newHEXACO_PI_key.htm){target="_blank"}) predict music preference.  


:::: {.columns}
::: {.column width="50%"}

For the data here I selected only 3 personality facets and 1 dimension of music preference.



<ul style="font-size: 22px">  

<li> **Intense**: Preference for the *intense* music dimension, which includes genres such as rock, punk, metal. </li>

<li> **Advnt**: Adventurousness facet of big 5 openness to experience. </li>

<li> **Intel**: Intellect facet of big 5 openness to experience. </li>

<li> **Uncon**: Unconventionality facet of HEXACO openness to experience. </li>


Higher values mean higher preference/levels of personality trait

</ul>

:::

::: {.column width="50%"}

::: {.callout-note}
## Dimensions of Music Preference ðŸŽ¶

@Rentfrow_Gosling_2003 was a seminal study in the field of music preference, where it was found that preference for music genres cluster together (e.g., if you like *punk* music, you tend to like *metal* music). Later @Rentfrow_etal_2011 proposed a 5-factor structure of music preference: *sophisticated* (e.g., classical, blues, jazz), *unpretentious* (e.g., country and folk), intense (e.g, rock, punk, metal), mellow (e.g., pop, electronic), and *contemporary* (e.g., rap, RnB). This 5-factor structure seems to work fairly well, although I would suspect that 1 or 2 more factors would emerge with a more comprehensive selection of music genres. 

:::

:::
::::


This data fits the lab topic well because in the paper we had to figure out a way of dealing with high multicollinearity, and dominance analysis ended up helping a lot!

## Regression coefficients Weirdness 

<div style="font-size: 24px"> It is common practice to add covariates to regression models without giving it much thought (*adding a covariate* simply means adding additional predictors). However, some strange things ðŸ‘½ may happen if you add variables too casually to regression models.
 </div>



:::: {.columns}
::: {.column width="50%"}

<div style="font-size: 22px; padding-top: 6px;"> For now, let's see how Adventurousness and intellect relate preference for Intense music: </div>


```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

round(cor(music[,1:3]), 2)
```

<div style="font-size: 22px;"> The correlations are all positive. </div>


<br>


<div style="font-size: 22px;">  But wait, the relation between `Advnt` and `Intense` becomes negative after accounting for `Intel` ðŸ™€ </div>

:::
::: {.column width="50%"}

<div style="font-size: 22px"> Let's say we want to know the relation between `Advnt` and `Intense` while accounting for `Intel`. We run a multiple regression: </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-115
#| 
reg_coll <- lm(Intense ~ Advnt + Intel, data = music)

# car function for shorter regression summary 
car::S(reg_coll, brief = TRUE)
```


:::
::::

## Visulazing Multicollinearity

When strange things happen, visualizing your data (if possible) is always a good idea. 

:::: {.columns}
::: {.column width="35%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-115

library(FabioFun)

nice_3D_plot(y = music$Intense,
             x1 = music$Advnt,
             x2 = music$Intel,
             axis_names = c("Intese", 
                            "Adventurousness", 
                            "Intellect"),
             reg_plane = TRUE)
```

<div style="font-size: 22px">  One apparent issue is that `Advnt` and `Intel` are extremely correlated ($r = .96$). You can see it from the 3D plot, which is essentially a 2D plot (`Advnt` and `Intel` are effectively a single variable, not two separate ones). This is a case of **multicollinearity**. </div>


<div style="font-size: 22px; padding-top: 12px;">  We are asking for a 3D plane when we should just ask for a line. In such cases, strange things are bound to happen. </div>


:::
::: {.column width="65%"}

```{r}
#| eval: true
#| echo: false 
#| code-line-numbers: false
#| classes: code-125

library(FabioFun)

nice_3D_plot(y = music$Intense,
             x1 = music$Advnt,
             x2 = music$Intel,
             axis_names = c("Intese", "Adventurousness", "Intellect"),
             reg_plane = TRUE) %>%  
  bslib::card(full_screen = TRUE)
```

:::
::::


## Back to Semi-Partial Correlations

If you remember, [semi-partial correlations](https://raw.githack.com/quinix45/PSYC-7804-Regression-Lab-Slides/main/Slides%20Files/Lab%206.html#/semi-partial-correlation-by-hand){target="_blank"} calculate the correlation between $y$ and $x$ after taking out the explained variance of $z$ from only one variable.


:::: {.columns}
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

ppcor::spcor(music[,1:3])[1]

```
We want to look at the first row. We can see that the relation between `Intense` and `advnt` is negative after the variance explained by `Intel` is taken out of `Advnt`.  

As we saw a few slides back, by themselves both variables are positively correlated with `Intense`


```{r}
#| classes: code-125

cor(music[,1:3])[1,]
```


:::
::: {.column width="30%"}

::: {.callout-note}

## A Word of Advice

When the sign of correlations and semi-partial correlations are different, something strange is happening. Multicollinearity may be the reason, but other possibilities exist (see [here](https://freerangestats.info/blog/2023/06/04/causality-sims){target="_blank"}). 

::: 

:::
::::


## Variance Inflation Factor


Probably the quickest way to check for multicollinearity is to calculate the variance inflation factor (VIF) for each predictor. For a predictor $x$, the formula is:


$$VIF_x = \frac{1}{1 - R^2}$$

Importantly, the $R^2$ in the VIF formula stands for the variance explained in the predictor $x$ by *all other predictors*. 

:::: {.columns}
::: {.column width="70%"}

We use the `vif()` function from `car` to get the VIF for our regression variables 

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

car::vif(reg_coll)
```


<div style="font-size: 22px"> A VIF higher than 10 usually suggest sizable multicollinearity between predictors. </div>

:::

::: {.column width="30%"}


::: {.callout-note}
## Question?

Why is the VIF for our two variables the same?  
::: 

:::

::::


## Another Perspective: Residuals

As shown in [Lab 5](https://raw.githack.com/quinix45/PSYC-7804-Regression-Lab-Slides/main/Slides%20Files/Lab%205.html#/calculate-partial-regression-coefficients-by-hand){target="_blank"}, regression coefficients are calculated based on the residuals of the predictors after controlling for all other predictors. But, if `Advnt` and `Intel` are so correlated, what happens to their residual after controlling fo one another? 

:::: {.columns}
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

resid_Advnt <- residuals(lm(Advnt ~ Intel, music))

var(resid_Advnt)
```

:::
::: {.column width="50%"}
```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

resid_Intel <- residuals(lm(Intel ~ Advnt, music))

var(resid_Intel)
```
:::
::::

<br>

<div style="font-size: 22px"> The residuals have almost no variance left! If a variable has variance of 0, it is always the same number (i.e., a constant), and cannot predict anything. A consequence of multicollinearity is that the regression coefficients that are calculated based on these residuals **should not be interpreted**. 
 </div>
 
<center>

However, $R^2$ is untouched by multicollinearity! So, you can still interpret $R^2$ just fine with high multicollinearity.

</center>

## Dominance Analysis

Dominance analysis  [@Budescu_1993; @Azen_Budescu_2003] main application is to determine the *relative* importance of a set of predictor variables. DA provides a way of *ranking* predictors by importance.   


:::: {.columns}
::: {.column width="70%"}

<div style="font-size: 24px"> Let's say we have three predictors, $x_1$, $x_2$ and $x_3$. We want know which one is the most important relative to the other 2 predictors. DA looks at how much each variable adds to $R^2$ when it is added to *all possible regression subsets*. </div>

<br>

<div style="font-size: 24px">  For Example, for $x_1$, we would look at how much the $R^2$ increases when we add it to a regression (1) with no variables , (2) only with $x_2$, (3) only with $x_3$, and (4) with $x_2$ and $x_3$.  </div>

<br>

Different dominance patterns (complete dominance, conditional dominance, general dominance) between variables can be established depending on the results of DA.

:::
::: {.column width="30%"}

<br>

<center>
<figure>
  <img src="Images/David_Budescu.jpg">
  <figcaption style="font-size: 16px"> [David Budescu](https://scholar.google.com/citations?user=f8zVLj8AAAAJ&hl=en){target="_blank"}, father of dominance analysis and Fordham professor who just retired. You may have seen him roam the halls of Dealy ðŸ‘€ 
  </figcaption>
</figure>
</center>

:::
::::


## Example with our Data

<div style="font-size: 24px"> In the case of our data we have 3 facets of openness to experience: adventurousness, intellect, and unconventionality. We want to know how these three variables is the most important in predicting preference for *Intense* music.  </div>


:::: {.columns}
::: {.column width="60%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

reg_da <- lm(Intense ~ Advnt + Intel + Uncon, music) 
DA <- dominanceAnalysis(reg_da)
summary(DA)
```


:::
::: {.column width="40%"}


<ul style="font-size: 22px">  

let's go over the meaning of the columns in the output

<li>  `model`: variables in the regression.  </li>
<li>  `level`: number of variables in the regression. </li>
<li>  `fit`: $R^2$ value for given regression. </li>
<li>  `remaining columns`: contribution in $R^2$ when adding variable in column. </li>
</ul>




<div style="font-size: 22px"> We see that across all possible regressions, `Uncon` contributes the most $R^2$ on average, $.064$ (i.e., general dominance). </div>

<div style="font-size: 18px; padding-top: 14px;"> Note the the $R^2$ values in the average contribution add up to the $R^2$ of the regression with all variables: </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

summary(reg_da)$r.squared
```


:::
::::


## Additional Dominance Patterns

So, on average `Uncon` is the most important predictor or preference for *Intense* music. This is just on average across all possible regression. We can build dominance matrices with *j* rows and *i* columns.


A dominance matrix will contain a **1** if the variable in the row $j$ dominates the one in column $i$. You will see a **.5** if the dominance pattern between two variables cannot be established. 


:::: {.columns}
::: {.column width="50%"}

<center> **Conditional Dominance** </center>

<div style="font-size: 22px"> We can check whether `Uncon` conditionally dominates the other variables, if on average it contributes more to $R^2$ across all the levels in the `level` column: </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

dominanceMatrix(DA, type = "conditional")
```

:::
::: {.column width="50%"}

<center> **Complete Dominance** </center>


<div style="font-size: 22px"> We can check whether `Uncon` completely dominates the other variables, if it contributes more to $R^2$ across **all regressions**: </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

dominanceMatrix(DA, type = "complete")
```

:::
::::

In both cases, `Uncon` dominates the other two variables. We now have a really strong case to claim that `Uncon` is the most important predictor of preference for *Intense* music among our 3 openness facets.

## Uncertainty and Inference in DA

As it is the case with any statistical procedure, our results are based on a sample, and there is uncertainty about our results (e.g., are they due to sampling error?). We can use bootstrap to check how often the dominance pattern replicates across every bootstrap sample (1000 in this case):


:::: {.columns}
::: {.column width="60%"}


```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125


set.seed(2341)
DA_boot <- bootDominanceAnalysis(reg_da, R = 1000)
summary(DA_boot)
```


```{r}
DA_boot <- readRDS("Additional files/DA_boot.RDS")
summary(DA_boot)
```

:::
::: {.column width="40%"}

Skipping over some stuff, the column that we care most about is the `rep` column. 

This column shows the proportion of bootstrap samples where the dominance pattern in the row was replicated. In general, `Uncon` seems to fairly consistently dominate all other variables.  

:::
::::


::: {.callout-note}
## DA and Multicollinearity

<div style="font-size: 18px"> If you remember $R^2$ is unaffected by multicollinearity. By extension, DA is unaffected too, as it only deals with $R^2$. Thus, DA can be a good way of making inferences about your variables when your regression coefficients are not interpretable due to high multicollinearity (which is why it came up in my case!) </div>
:::

## References 

<div id="refs"> </div>


# Power Analysis in Multiple Regression


## What is Powerâš¡?

**Power:** Statistical power is defined as *the probability of correctly rejecting the null hypothesis if $H_0$ is false*. Power is also known as 1 - Type II error rate. Type II error rate is denoted with $\beta$, and is the probability of failing to reject a false $H_0$. 

In practice, power is used is mostly to answer the question: "how big of a sample size do I need to get a *p*-value lower than .05 given a certain effect size?" 


:::: {.columns}
::: {.column width="50%"}

</br>

<div style="font-size: 24px"> The probability of making a Type I error is always the level of significance, $\alpha$. As you know, the level of significance is almost unilaterally $.05$. So, unless specified otherwise,  $\alpha = .05$. </div>


<div style="font-size: 24px"> On the other hand, it is common to generally look for a sample size that gives a power of $\beta = .8$. This would mean that if $H_0$ is false, you would reject it $80\%$ of the times. </div>


:::
::: {.column width="50%"}

![](Images/type-i-and-ii-error-2.png)


:::
::::


## Power in Regresison

In regression, power is 


## Personal opinions and Rant 

Feel free to ignore this slide, but...I personally *really, strongly, dislike* the idea of power and how it is used. In no particular order:


:::: {.columns}
::: {.column width="50%"}

<center> **Theoretical Reasons:** </center>

- The answer is always get as high of a sample size as you can.

- Type I and type II error rates are not real. We should focus on other things (see [here](https://statmodeling.stat.columbia.edu/2004/12/29/type_1_type_2_t/){target="_blank"}).

- Why do I care how many people I need to get $p < .05$? (no seriously, think about it)



:::
::: {.column width="50%"}

<center> **Practical Reasons:** </center>

- people who will ask you to do power analysis, will do it becasue:



:::
::::



















