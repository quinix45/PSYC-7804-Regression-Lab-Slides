@online{_,
  title = {Shiny - Build a User Interface},
  url = {https://shiny.posit.co/r/getstarted/shiny-basics/lesson2/},
  urldate = {2024-01-16},
  abstract = {Shiny is a package that makes it easy to create interactive web apps using R and Python.},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\fabio\Zotero\storage\DLZBFP9U\lesson2.html}
}

@online{_2024,
  title = {Slidecraft 101},
  date = {2024-06-05},
  url = {https://emilhvitfeldt.com/project/slidecraft-101/#blog-posts},
  urldate = {2024-12-09},
  abstract = {All things slidecraft with Quarto Revealjs slides},
  langid = {english},
  keywords = {/unread},
  file = {C:\Users\fabio\Zotero\storage\NB6X98D8\slidecraft-101.html}
}

@online{_g,
  title = {Simsem/{{Conference}} Presentations/{{JorgensenSchoemann}}.2023.{{simsem4LSA}}.Pdf at Master · Simsem/Simsem},
  url = {https://github.com/simsem/simsem/blob/master/Conference%20presentations/JorgensenSchoemann.2023.simsem4LSA.pdf},
  urldate = {2024-05-14},
  file = {C:\Users\fabio\Zotero\storage\GTXEVBBY\simsemConference presentationsJorgensenSchoemann.pdf}
}

@online{_i,
  title = {R: {{Mathematical Annotation}} in {{R}}},
  url = {https://astrostatistics.psu.edu/su07/R/html/grDevices/html/plotmath.html},
  urldate = {2024-05-26},
  keywords = {/unread},
  file = {C:\Users\fabio\Zotero\storage\4F5IRFZ4\plotmath.html}
}

@software{_l,
  title = {Introduction},
  url = {https://cran.r-project.org/web/packages/posterior/vignettes/posterior.html},
  urldate = {2024-06-15},
  abstract = {The posterior R package is intended to provide useful tools for both users and developers of packages for fitting Bayesian models or working with output from Bayesian models. The primary goals of the package are to:},
  keywords = {/unread}
}

@online{_p,
  title = {Statistics 5101 ({{Geyer}}, {{Spring}} 2022)},
  url = {https://www.stat.umn.edu/geyer/5101/},
  urldate = {2024-12-10},
  keywords = {/unread},
  file = {C:\Users\fabio\Zotero\storage\2E3YXB8E\5101.html}
}

@online{_s,
  title = {Statistical Procedures and the Justification of Knowldge in Psychological Science - {{Google Search}}},
  url = {https://www.google.com/search?client=opera-gx&q=statistical+procedures+and+the+justification+of+knowldge+in+psychological+science&sourceid=opera&ie=UTF-8&oe=UTF-8},
  urldate = {2024-12-25},
  keywords = {/unread}
}

@software{Almeida_etal_2023,
  title = {Qqplotr: {{Quantile-Quantile Plot Extensions}} for 'Ggplot2'},
  shorttitle = {Qqplotr},
  author = {Almeida, Alexandre and Loy, Adam and Hofmann, Heike},
  date = {2023-01-25},
  url = {https://cran.r-project.org/web/packages/qqplotr/index.html},
  urldate = {2024-12-24},
  abstract = {Extensions of 'ggplot2' Q-Q plot functionalities.},
  version = {0.0.6},
  keywords = {/unread}
}

@software{Becker_etal_2024,
  title = {Rio: {{A Swiss-Army Knife}} for {{Data I}}/{{O}}},
  shorttitle = {Rio},
  author = {Becker, Jason and Chan, Chung-hong and Schoch, David and Chan, Geoffrey CH and Leeper, Thomas J. and Gandrud, Christopher and MacDonald, Andrew and Zahn, Ista and Stadlmann, Stanislaus and Williamson, Ruaridh and Kennedy, Patrick and Price, Ryan and Davis, Trevor L. and Day, Nathan and Denney, Bill and Bokov, Alex and Gruson, Hugo},
  date = {2024-09-25},
  url = {https://cran.r-project.org/web/packages/rio/index.html},
  urldate = {2024-12-20},
  abstract = {Streamlined data import and export by making assumptions that the user is probably willing to make: 'import()' and 'export()' determine the data format from the file extension, reasonable defaults are used for data import and export, web-based import is natively supported (including from SSL/HTTPS), compressed files can be read directly, and fast import packages are used where appropriate. An additional convenience function, 'convert()', provides a simple method for converting between file types.},
  version = {1.2.3},
  keywords = {/unread,WebTechnologies}
}

@software{Becker_etal_2024a,
  title = {{{PearsonDS}}: {{Pearson Distribution System}}},
  shorttitle = {{{PearsonDS}}},
  author = {Becker, Martin and Klößner, Stefan and Heinrich, Joel},
  date = {2024-02-08},
  url = {https://cran.r-project.org/web/packages/PearsonDS/index.html},
  urldate = {2024-12-20},
  abstract = {Implementation of the Pearson distribution system, including full support for the (d,p,q,r)-family of functions for probability distributions and fitting via method of moments and maximum likelihood method.},
  version = {1.3.1},
  keywords = {/unread,Distributions}
}

@article{Bengtsson_2021,
  title = {A {{Unifying Framework}} for {{Parallel}} and {{Distributed Processing}} in {{R}} Using {{Futures}}},
  author = {Bengtsson, Henrik},
  date = {2021},
  journaltitle = {The R Journal},
  shortjournal = {The R Journal},
  volume = {13},
  number = {2},
  eprint = {2008.00553},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  pages = {208},
  issn = {2073-4859},
  doi = {10.32614/RJ-2021-048},
  url = {http://arxiv.org/abs/2008.00553},
  urldate = {2024-06-02},
  abstract = {A future is a programming construct designed for concurrent and asynchronous evaluation of code, making it particularly useful for parallel processing. The future package implements the Future API for programming with futures in R. This minimal API provides sufficient constructs for implementing parallel versions of well-established, high-level map-reduce APIs. The future ecosystem supports exception handling, output and condition relaying, parallel random number generation, and automatic identification of globals lowering the threshold to parallelize code. The Future API bridges parallel frontends with parallel backends following the philosophy that end-users are the ones who choose the parallel backend while the developer focuses on what to parallelize. A variety of backends exist and third-party contributions meeting the specifications, which ensure that the same code works on all backends, are automatically supported. The future framework solves several problems not addressed by other parallel frameworks in R.},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Statistics - Computation},
  file = {C\:\\Users\\fabio\\Zotero\\storage\\5782ZFS4\\Bengtsson - 2021 - A Unifying Framework for Parallel and Distributed .pdf;C\:\\Users\\fabio\\Zotero\\storage\\PML2FWI6\\2008.html}
}

@article{Chalmers_2012,
  title = {Mirt: A Multidimensional Item Response Theory Package for the {{R}} Environment},
  shorttitle = {Mirt},
  author = {Chalmers, R. Philip},
  date = {2012-05-24},
  journaltitle = {Journal of Statistical Software},
  volume = {48},
  pages = {1--29},
  issn = {1548-7660},
  doi = {10.18637/jss.v048.i06},
  url = {https://doi.org/10.18637/jss.v048.i06},
  urldate = {2023-11-25},
  abstract = {Item response theory (IRT) is widely used in assessment and evaluation research to explain how participants respond to item level stimuli. Several R packages can be used to estimate the parameters in various IRT models, the most flexible being the ltm (Rizopoulos 2006), eRm (Mair and Hatzinger 2007), and MCMCpack (Martin, Quinn, and Park 2011) packages. However these packages have limitations in that ltm and eRm can only analyze unidimensional IRT models effectively and the exploratory multidimensional extensions available in MCMCpack requires prior understanding of Bayesian estimation convergence diagnostics and are computationally intensive. Most importantly, multidimensional confirmatory item factor analysis methods have not been implemented in any R package. The mirt package was created for estimating multidimensional item response theory parameters for exploratory and confirmatory models by using maximum-likelihood meth- ods. The Gauss-Hermite quadrature method used in traditional EM estimation (e.g., Bock and Aitkin 1981) is presented for exploratory item response models as well as for confirmatory bifactor models (Gibbons and Hedeker 1992). Exploratory and confirmatory models are estimated by a stochastic algorithm described by Cai (2010a,b). Various program comparisons are presented and future directions for the package are discussed.},
  langid = {english},
  keywords = {mirt},
  file = {C:\Users\fabio\Zotero\storage\55PA7NJR\Chalmers - 2012 - mirt A Multidimensional Item Response Theory Pack.pdf}
}

@software{kimPpcorPartialSemiPartial2015,
  title = {Ppcor: {{Partial}} and {{Semi-Partial}} ({{Part}}) {{Correlation}}},
  shorttitle = {Ppcor},
  author = {Kim, Seongho},
  year = {2015},
  month = dec,
  urldate = {2025-01-01},
  abstract = {Calculates partial and semi-partial (part) correlations along with p-value.},
  copyright = {GPL-2}
}

@article{akaikeNewLookStatistical1974,
  title = {A New Look at the Statistical Model Identification},
  author = {Akaike, H.},
  year = {1974},
  month = dec,
  journal = {IEEE Transactions on Automatic Control},
  volume = {19},
  number = {6},
  pages = {716--723},
  issn = {1558-2523},
  doi = {10.1109/TAC.1974.1100705},
  urldate = {2024-03-15},
  abstract = {The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.},
  keywords = {Art,Estimation theory,History,Linear systems,Maximum likelihood estimation,Roundoff errors,Sampling methods,Stochastic processes,Testing,Time series analysis},
  file = {C:\Users\fabio\Zotero\storage\LCS4YGYE\1100705.html}
}

@article{schwarzEstimatingDimensionModel1978,
  title = {Estimating the {{Dimension}} of a {{Model}}},
  author = {Schwarz, Gideon},
  year = {1978},
  month = mar,
  journal = {The Annals of Statistics},
  volume = {6},
  number = {2},
  pages = {461--464},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176344136},
  urldate = {2024-03-15},
  abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
  keywords = {62F99,62J99,Akaike information criterion,asymptotics,dimension},
  file = {C:\Users\fabio\Zotero\storage\C7ZWVZBI\Schwarz - 1978 - Estimating the Dimension of a Model.pdf}
}


@article{kimPpcorPackageFast2015,
  title = {Ppcor: {{An R Package}} for a {{Fast Calculation}} to {{Semi-partial Correlation Coefficients}}},
  shorttitle = {Ppcor},
  author = {Kim, Seongho},
  year = {2015},
  month = nov,
  journal = {Communications for statistical applications and methods},
  volume = {22},
  number = {6},
  pages = {665--674},
  issn = {2287-7843},
  doi = {10.5351/CSAM.2015.22.6.665},
  urldate = {2025-01-02},
  abstract = {Lack of a general matrix formula hampers implementation of the semi-partial correlation, also known as part correlation, to the higher-order coefficient. This is because the higher-order semi-partial correlation calculation using a recursive formula requires an enormous number of recursive calculations to obtain the correlation coefficients. To resolve this difficulty, we derive a general matrix formula of the semi-partial correlation for fast computation. The semi-partial correlations are then implemented on an R package ppcor along with the partial correlation. Owing to the general matrix formulas, users can readily calculate the coefficients of both partial and semi-partial correlations without computational burden. The package ppcor further provides users with the level of the statistical significance with its test statistic.},
  pmcid = {PMC4681537},
  pmid = {26688802},
  file = {C:\Users\fabio\Zotero\storage\3RR63XCT\Kim - 2015 - ppcor An R Package for a Fast Calculation to Semi-partial Correlation Coefficients.pdf}
}


@software{Fox_etal_2024,
  title = {Car: {{Companion}} to {{Applied Regression}}},
  shorttitle = {Car},
  author = {Fox, John and Weisberg, Sanford and Price, Brad and Adler, Daniel and Bates, Douglas and Baud-Bovy, Gabriel and Bolker, Ben and Ellison, Steve and Firth, David and Friendly, Michael and Gorjanc, Gregor and Graves, Spencer and Heiberger, Richard and Krivitsky, Pavel and Laboissiere, Rafael and Maechler, Martin and Monette, Georges and Murdoch, Duncan and Nilsson, Henric and Ogle, Derek and Ripley, Brian and Short, Tom and Venables, William and Walker, Steve and Winsemius, David and Zeileis, Achim and R-Core},
  date = {2024-09-27},
  url = {https://cran.r-project.org/web/packages/car/index.html},
  urldate = {2024-12-20},
  abstract = {Functions to Accompany J. Fox and S. Weisberg, An R Companion to Applied Regression, Third Edition, Sage, 2019.},
  version = {3.1-3},
  keywords = {/unread,Econometrics,Finance,MixedModels,TeachingStatistics}
}

@software{Gohel_etal_2024,
  title = {Flextable: {{Functions}} for {{Tabular Reporting}}},
  shorttitle = {Flextable},
  author = {Gohel, David and ArData and Jager, Clementine and Daniels, Eli and Skintzos, Panagiotis and Fazilleau, Quentin and Nazarov, Maxim and Robert, Titouan and Barrowman, Michael and Yasumoto, Atsushi and Julian, Paul and Browning, Sean and Thériault, Rémi and Jobert, Samuel and Newman, Keith},
  date = {2024-10-27},
  url = {https://cran.r-project.org/web/packages/flextable/index.html},
  urldate = {2024-12-24},
  abstract = {Use a grammar for creating and customizing pretty tables. The following formats are supported: 'HTML', 'PDF', 'RTF', 'Microsoft Word', 'Microsoft PowerPoint' and R 'Grid Graphics'. 'R Markdown', 'Quarto' and the package 'officer' can be used to produce the result files. The syntax is the same for the user regardless of the type of output to be produced. A set of functions allows the creation, definition of cell arrangement, addition of headers or footers, formatting and definition of cell content with text and or images. The package also offers a set of high-level functions that allow tabular reporting of statistical models and the creation of complex cross tabulations.},
  version = {0.9.7},
  keywords = {/unread,ReproducibleResearch}
}

@article{Mazza_etal_2014,
  title = {{{KernSmoothIRT}}: An {{R}} Package for Kernel Smoothing in Item Response Theory},
  shorttitle = {{{KernSmoothIRT}}},
  author = {Mazza, Angelo and Punzo, Antonio and McGuire, Brian},
  date = {2014-06-30},
  journaltitle = {Journal of Statistical Software},
  volume = {58},
  pages = {1--34},
  issn = {1548-7660},
  doi = {10.18637/jss.v058.i06},
  url = {https://doi.org/10.18637/jss.v058.i06},
  urldate = {2024-01-04},
  abstract = {Item response theory (IRT) models are a class of statistical models used to describe the response behaviors of individuals to a set of items having a certain number of options. They are adopted by researchers in social science, particularly in the analysis of performance or attitudinal data, in psychology, education, medicine, marketing and other fields where the aim is to measure latent constructs. Most IRT analyses use parametric models that rely on assumptions that often are not satisfied. In such cases, a nonparametric approach might be preferable; nevertheless, there are not many software implementations allowing to use that. To address this gap, this paper presents the R  package KernSmoothIRT . It implements kernel smoothing for the estimation of option characteristic curves, and adds several plotting and analytical tools to evaluate the whole test/questionnaire, the items, and the subjects. In order to show the package's capabilities, two real datasets are used, one employing multiple-choice responses, and the other scaled responses.},
  langid = {english},
  keywords = {KernSmoothIRT},
  file = {C:\Users\fabio\Zotero\storage\CVSLIPWT\Mazza et al. - 2014 - KernSmoothIRT An R Package for Kernel Smoothing i.pdf}
}

@article{Merkle_Rosseel_2018,
  title = {Blavaan: {{Bayesian Structural Equation Models}} via {{Parameter Expansion}}},
  shorttitle = {Blavaan},
  author = {Merkle, Edgar C. and Rosseel, Yves},
  date = {2018-06-10},
  journaltitle = {Journal of Statistical Software},
  volume = {85},
  pages = {1--30},
  issn = {1548-7660},
  doi = {10.18637/jss.v085.i04},
  url = {https://doi.org/10.18637/jss.v085.i04},
  urldate = {2024-05-01},
  abstract = {This article describes blavaan, an R package for estimating Bayesian structural equation models (SEMs) via JAGS and for summarizing the results. It also describes a novel parameter expansion approach for estimating specific types of models with residual covariances, which facilitates estimation of these models in JAGS. The methodology and software are intended to provide users with a general means of estimating Bayesian SEMs, both classical and novel, in a straightforward fashion. Users can estimate Bayesian versions of classical SEMs with lavaan syntax, they can obtain state-of-the-art Bayesian fit measures associated with the models, and they can export JAGS code to modify the SEMs as desired. These features and more are illustrated by example, and the parameter expansion approach is explained in detail.},
  langid = {english},
  keywords = {Bayesian SEM,JAGS,lavaan,MCMC,structural equation models},
  file = {C:\Users\fabio\Zotero\storage\ERH6LX4P\Merkle and Rosseel - 2018 - blavaan Bayesian Structural Equation Models via P.pdf}
}

@manual{RCoreTeam_2024,
  type = {manual},
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  date = {2024},
  publisher = {R Foundation for Statistical Computing},
  location = {Vienna, Austria},
  url = {https://www.R-project.org/},
  keywords = {/unread}
}

@article{Rees_,
  title = {Biblatex Cheat Sheet},
  author = {Rees, Clea F},
  langid = {english},
  file = {C:\Users\fabio\Zotero\storage\ZDYXVB9M\Rees - Biblatex Cheat Sheet.pdf}
}

@article{Revelle_,
  title = {Using the Psych Package to Generate and Test Structural Models},
  author = {Revelle, William},
  langid = {english},
  file = {C:\Users\fabio\Zotero\storage\2GBAAB54\Revelle - Using the psych package to generate and test struc.pdf}
}

@software{Revelle_2024,
  title = {{{psychTools}}: {{Tools}} to {{Accompany}} the 'psych' {{Package}} for {{Psychological Research}}},
  shorttitle = {{{psychTools}}},
  author = {Revelle, William},
  date = {2024-03-19},
  url = {https://cran.r-project.org/web/packages/psychTools/index.html},
  urldate = {2024-07-02},
  abstract = {Support functions, data sets, and vignettes for the 'psych' package. Contains several of the biggest data sets for the 'psych' package as well as four vignettes. A few helper functions for file manipulation are included as well. For more information, see the {$<$}https://personality-project.org/r/{$>$} web page.},
  version = {2.4.3},
  keywords = {/unread,Psychometrics}
}

@software{Revelle_2024a,
  title = {Psych: {{Procedures}} for {{Psychological}}, {{Psychometric}}, and {{Personality Research}}},
  shorttitle = {Psych},
  author = {Revelle, William},
  date = {2024-06-27},
  url = {https://cran.r-project.org/web/packages/psych/index.html},
  urldate = {2024-12-20},
  abstract = {A general purpose toolbox developed originally for personality, psychometric theory and experimental psychology. Functions are primarily for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis, although others provide basic descriptive statistics. Item Response Theory is done using factor analysis of tetrachoric and polychoric correlations. Functions for analyzing data at multiple levels include within and between group statistics, including correlations and factor analysis. Validation and cross validation of scales developed using basic machine learning algorithms are provided, as are functions for simulating and testing particular item and test structures. Several functions serve as a useful front end for structural equation modeling. Graphical displays of path diagrams, including mediation models, factor analysis and structural equation models are created using basic graphics. Some of the functions are written to support a book on psychometric theory as well as publications in personality research. For more information, see the {$<$}https://personality-project.org/r/{$>$} web page.},
  version = {2.4.6.26},
  keywords = {/unread,Psychometrics}
}

@article{Rosnow_Rosenthal_1989,
  title = {Statistical Procedures and the Justification of Knowledge in Psychological Science},
  author = {Rosnow, Ralph L. and Rosenthal, Robert},
  date = {1989},
  journaltitle = {American Psychologist},
  volume = {44},
  number = {10},
  pages = {1276--1284},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1935-990X},
  doi = {10.1037/0003-066X.44.10.1276},
  abstract = {Justification, in the vernacular language of philosophy of science, refers to the evaluation, defense, and confirmation of claims of truth. In this article, we examine some aspects of the rhetoric of justification, which in part draws on statistical data analysis to shore up facts and inductive inferences. There are a number of problems of methodological spirit and substance that in the past have been resistant to attempts to correct them. The major problems are discussed, and readers are reminded of ways to clear away these obstacles to justification. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {/unread,Methodology,Psychology,Statistical Analysis},
  file = {C:\Users\fabio\Zotero\storage\ZSBKU7NR\1990-00198-001.html}
}

@software{Rosseel_etal_2024,
  title = {Lavaan: {{Latent Variable Analysis}}},
  shorttitle = {Lavaan},
  author = {Rosseel, Yves and Jorgensen, Terrence D. and Wilde, Luc De and Oberski, Daniel and Byrnes, Jarrett and Vanbrabant, Leonard and Savalei, Victoria and Merkle, Ed and Hallquist, Michael and Rhemtulla, Mijke and Katsikatsou, Myrsini and Barendse, Mariska and Rockwood, Nicholas and Scharf, Florian and Du, Han and Jamil, Haziq and Classe, Franz},
  date = {2024-09-26},
  url = {https://cran.r-project.org/web/packages/lavaan/index.html},
  urldate = {2024-11-13},
  abstract = {Fit a variety of latent variable models, including confirmatory factor analysis, structural equation modeling and latent growth curve models.},
  version = {0.6-19},
  keywords = {/unread,Econometrics,MissingData,MixedModels,Psychometrics}
}

@software{Schloerke_etal_2024,
  title = {{{GGally}}: {{Extension}} to 'Ggplot2'},
  shorttitle = {{{GGally}}},
  author = {Schloerke, Barret and Cook, Di and Larmarange, Joseph and Briatte, Francois and Marbach, Moritz and Thoen, Edwin and Elberg, Amos and Toomet, Ott and Crowley, Jason and Hofmann, Heike and Wickham, Hadley},
  date = {2024-02-14},
  url = {https://cran.r-project.org/web/packages/GGally/index.html},
  urldate = {2024-12-27},
  abstract = {The R package 'ggplot2' is a plotting system based on the grammar of graphics. 'GGally' extends 'ggplot2' by adding several functions to reduce the complexity of combining geometric objects with transformed data. Some of these functions include a pairwise plot matrix, a two group pairwise plot matrix, a parallel coordinates plot, a survival plot, and several functions to plot networks.},
  version = {2.2.1},
  keywords = {/unread}
}

@incollection{efronBootstrapMethodsAnother1977,
  title = {Bootstrap {{Methods}}: {{Another Look}} at the {{Jackknife}}},
  shorttitle = {Bootstrap {{Methods}}},
  booktitle = {Breakthroughs in {{Statistics}}: {{Methodology}} and {{Distribution}}},
  author = {Efron, Bradley},
  editor = {Kotz, Samuel and Johnson, Norman L.},
  year = {1977},
  pages = {569--593},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-4380-9_41},
  urldate = {2024-12-31},
  abstract = {We discuss the following problem given a random sample X = (X1, X2,{\dots}, Xn) from an unknown probability distribution F, estimate the sampling distribution of some prespecified random variable R(X, F), on the basis of the observed data x. (Standard jackknife theory gives an approximate mean and variance in the case R(X, F) = \$\${\textbackslash}theta {\textbackslash}left( \{{\textbackslash}hat F\} {\textbackslash}right) - {\textbackslash}theta {\textbackslash}left( F {\textbackslash}right)\$\$, {\texttheta} some parameter of interest.) A general method, called the ``bootstrap'', is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
  isbn = {978-1-4612-4380-9},
  langid = {english}
}


@software{Sievert_etal_2024,
  title = {Bslib: {{Custom}} '{{Bootstrap}}' '{{Sass}}' {{Themes}} for 'Shiny' and 'Rmarkdown'},
  shorttitle = {Bslib},
  author = {Sievert, Carson and Cheng, Joe and Aden-Buie, Garrick and Software, Posit and PBC and {library)}, Bootstrap contributors (Bootstrap and Twitter and {library)}, Inc (Bootstrap and family=library), given=Javi Aguilar (Bootstrap, prefix=colorpicker, useprefix=false and {library)}, Thomas Park (Bootswatch and family=plugin), given=PayPal (Bootstrap, prefix=accessibility, useprefix=false},
  date = {2024-07-29},
  url = {https://cran.r-project.org/web/packages/bslib/index.html},
  urldate = {2024-12-20},
  abstract = {Simplifies custom 'CSS' styling of both 'shiny' and 'rmarkdown' via 'Bootstrap' 'Sass'. Supports 'Bootstrap' 3, 4 and 5 as well as their various 'Bootswatch' themes. An interactive widget is also provided for previewing themes in real time.},
  version = {0.8.0},
  keywords = {/unread}
}

@software{Sievert_etal_2024a,
  title = {Plotly: {{Create Interactive Web Graphics}} via 'Plotly.Js'},
  shorttitle = {Plotly},
  author = {Sievert, Carson and Parmer, Chris and Hocking, Toby and Chamberlain, Scott and Ram, Karthik and Corvellec, Marianne and Despouy, Pedro and Brüggemann, Salim and Inc, Plotly Technologies},
  date = {2024-01-13},
  url = {https://cran.r-project.org/web/packages/plotly/index.html},
  urldate = {2024-12-27},
  abstract = {Create interactive web graphics from 'ggplot2' graphs and/or a custom interface to the (MIT-licensed) JavaScript library 'plotly.js' inspired by the grammar of graphics.},
  version = {4.10.4},
  keywords = {/unread,DynamicVisualizations,WebTechnologies}
}

@article{Textor_,
  title = {Drawing and {{Analyzing Causal DAGs}} with {{DAGitty}}},
  author = {Textor, Johannes},
  langid = {english},
  file = {C:\Users\fabio\Zotero\storage\ZGX5JYC4\Textor - Drawing and Analyzing Causal DAGs with DAGitty.pdf}
}

@software{Vehtari_etal_2024a,
  title = {Loo: {{Efficient Leave-One-Out Cross-Validation}} and {{WAIC}} for {{Bayesian Models}}},
  shorttitle = {Loo},
  author = {Vehtari, Aki and Gabry, Jonah and Magnusson, Mans and Yao, Yuling and Bürkner, Paul-Christian and Paananen, Topi and Gelman, Andrew and Goodrich, Ben and Piironen, Juho and Nicenboim, Bruno and Lindgren, Leevi},
  date = {2024-02-24},
  url = {https://cran.r-project.org/web/packages/loo/index.html},
  urldate = {2024-05-27},
  abstract = {Efficient approximate leave-one-out cross-validation (LOO) for Bayesian models fit using Markov chain Monte Carlo, as described in Vehtari, Gelman, and Gabry (2017) {$<$}doi:10.1007/s11222-016-9696-4{$>$}. The approximation uses Pareto smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. As a byproduct of the calculations, we also obtain approximate standard errors for estimated predictive errors and for the comparison of predictive errors between models. The package also provides methods for using stacking and other model weighting techniques to average Bayesian predictive distributions.},
  version = {2.7.0},
  keywords = {Bayesian},
  file = {C:\Users\fabio\Zotero\storage\BQY3DY3H\Vehtari et al. - 2024 - loo Efficient Leave-One-Out Cross-Validation and .pdf}
}

@book{Wickham_,
  title = {Welcome | Mastering Shiny},
  author = {Wickham, Hadley},
  url = {https://mastering-shiny.org/index.html},
  urldate = {2024-01-14},
  abstract = {A book created with bookdown.},
  langid = {english},
  file = {C:\Users\fabio\Zotero\storage\KYX3KXQG\index.html}
}

@software{Wickham_etal_2024,
  title = {Ggplot2: {{Create Elegant Data Visualisations Using}} the {{Grammar}} of {{Graphics}}},
  shorttitle = {Ggplot2},
  author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and Yutani, Hiroaki and Dunnington, Dewey and family=Brand, given=Teun, prefix=van den, useprefix=false and Posit and PBC},
  date = {2024-04-23},
  url = {https://cran.r-project.org/web/packages/ggplot2/index.html},
  urldate = {2024-12-21},
  abstract = {A system for 'declaratively' creating graphics, based on "The Grammar of Graphics". You provide the data, tell 'ggplot2' how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.},
  version = {3.5.1},
  keywords = {/unread,ChemPhys,Phylogenetics,Spatial,TeachingStatistics}
}

@software{Wickham_RStudio_2023,
  title = {Tidyverse: {{Easily Install}} and {{Load}} the '{{Tidyverse}}'},
  shorttitle = {Tidyverse},
  author = {Wickham, Hadley and RStudio},
  date = {2023-02-22},
  url = {https://cran.r-project.org/web/packages/tidyverse/index.html},
  urldate = {2024-12-20},
  abstract = {The 'tidyverse' is a set of packages that work in harmony because they share common data representations and 'API' design. This package is designed to make it easy to install and load multiple 'tidyverse' packages in a single step. Learn more about the 'tidyverse' at {$<$}https://www.tidyverse.org{$>$}.},
  version = {2.0.0},
  keywords = {/unread,ChemPhys}
}

@article{Zhu_,
  title = {Create Awesome {{LaTeX}} Table with Knitr::Kable and {{kableExtra}}},
  author = {Zhu, Hao},
  langid = {english},
  file = {C:\Users\fabio\Zotero\storage\H2UKEQFN\Zhu - Create Awesome LaTeX Table with knitrkable and k.pdf}
}
