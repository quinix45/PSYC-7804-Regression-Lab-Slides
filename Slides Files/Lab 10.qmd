---
title: "Lab 10: Categorical Predictors"
author: "Fabio Setti"
institute: "PSYC 7804 - Regression with Lab"
bibliography: Additional files/R packages.bib
csl: Additional files/apa.csl
notice: |
  @Wickham_RStudio_2023
title-slide-attributes:
  data-transition: "zoom"
  data-visibility: "uncounted"

format:
   revealjs:
      footer: "Lab 10: Categorical Predictors"
      width: 1280
      height: 720
      chalkboard: true
      revealjs-fontawesome: true
      slide-number: c/t 
      theme: Fabio_theme/Fabio_theme.scss
      navigation-mode: linear
      controls: false
      auto-stretch: false
      header-includes:
        - <script src="Fabio_theme/Fabio_theme.js"></script>

  
editor: source
---

## Font-awesome slide{visibility="hidden"}

uncounted and not visible slide to trigger the fontawesome extension

{{< fa thumbs-up >}} 


## Today's Packages and Data 🤗

:::: {.columns}
::: {.column width="50%"}

```{r}
#| code-fold: true
#| eval: false
#| echo: true
#| code-line-numbers: false
#| code-summary: "Install Packages Code"
#| classes: code-150

# install.packages("tidyverse")
install.packages("fastDummies")
```

```{r}
#| eval: true
#| echo: true
#| code-line-numbers: false
#| warning: false
#| classes: code-150

library(tidyverse)
theme_set(theme_classic(base_size = 14, 
                        base_family = 'serif'))
library(fastDummies)
```


</br>

<div style="font-size: 26px">

::: {.panel-tabset}
### `fastDummies`

<div style="font-size: 22px"> The `fastDummies` package [@Kaplan_Schlegel_2025] provides some functions to help with dummy coding. Later, I show a better way of achieving dummy coding in `R`, but this package offers the "more standard" procedure. </div>

:::

</div>

  

:::


::: {.column width="50%"}

<center style="padding-bottom: 41px;"> [Data]{.data-title} </center>

<div style="font-size: 22px"> This data come from a random sample of 200 students from the 1982 [high-school and beyond longitudinal study](https://nces.ed.gov/surveys/hsb/){target="_blank"}. See [here](https://www.openintro.org/data/index.php?data=hsb2){target="_blank"} to find out more about the variables. 
 </div>



```{r}
#| warning: false
#| classes: code-125
#| echo: true
#| code-line-numbers: false
#| output: true

hsb2 <- rio::import("https://github.com/quinix45/PSYC-7804-Regression-Lab-Slides/raw/refs/heads/main/Slides%20Files/Data/hsb2.csv")
```

<center style="padding-bottom: 11px;"> </center>

```{r}
reactable::reactable(hsb2,
                     style = list(fontFamily = "Work Sans, sans-serif", fontSize = "1.105rem"),
                     pagination = FALSE, highlight = TRUE, height = 300)
```


:::
::::


## Categorical Variables in Regression

So far we have only dealt with continuous variables. Namely, we have interpreted slopes as "the change in $Y$ per unit change in $X$". 


::: {.fragment fragment-index=1}
<div style="font-size: 24px"> This interpretation only makes sense if both $Y$ and $X$ are continuous. But if we want to say that in this data `gender` should predict `write` score, the "unit-change" notion does not make as much sense. </div>
:::



:::: {.columns}
::: {.column width="40%"}


::: {.fragment fragment-index=2}
<div style="font-size: 24px; padding-top: 12px;"> If we try to plot the `write` score on the $y$-axis and `gender` on the $x$-axis we can visualize observations in each group. But there are no "units" on the $x$-axis 🤔 </div>
:::


::: {.fragment fragment-index=3}
<div style="font-size: 26px; padding-top: 22px;"> However, it turns out that there are ways to trick regression into treating categorical variables as continuous! </div>
:::


::: {.fragment fragment-index=4}
<div style="font-size: 26px; padding-top: 12px;"> But first, I would like to point something out... </div>
:::

:::
::: {.column width="60%"}

::: {.fragment fragment-index=2}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot code"

ggplot(hsb2, aes(x = gender, y = write)) +
  geom_point()
```

:::


:::
::::

## Intercept only Regression



You can use the `lm()` function with only one variable. Meaning, you can run a regression without any predictors (**!?**). for example, for the `write` variable:

:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=1}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

car::S(lm(write ~ 1, data = hsb2))
```
:::

:::
::: {.column width="50%"}


::: {.fragment fragment-index=2}

The intercept, $52.78$ and the residual *SD*, $9.47$. These values are the **mean** and ***SD*** of the `write` variable. 

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

mean(hsb2$write)
sd(hsb2$write)

```

<div style="font-size: 20px"> This is consistent with the representation of regression shown in the [appendix of Lab 2](https://quinix45.github.io/PSYC-7804-Regression-Lab-Slides/Slides%20Files/Lab%202.html#/the-true-regression-model){target="_blank"}, Here $Y \sim N(\mu = 52.78, \sigma = 9.47)$
 </div>

:::

:::
::::

::: {.fragment fragment-index=3}
<div style="font-size: 24px; padding-top: 14px"> So regression with only the intercept estimates the mean and *SD* of $Y$. But what now? Let's look at the graphical representation of the intercept only model. </div>
:::


## Intercept only model Visualized

If we only use the intercept in out model, the "regression line" will be flat and intercept the $y$-axis at the mean of $Y$.

:::: {.columns}
::: {.column width="30%"}


::: {.fragment fragment-index=1}
...but wait a moment, we have two groups, `female` and `male`. How about we give each group their own intercept?
:::

</br>

::: {.fragment fragment-index=2}
- this simply implies that we believe that the mean of the $Y$ variable, `write`, should differ based on `gender`. 
:::

:::
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot code"

ggplot(hsb2, aes(x = gender, y = write)) +
  geom_point() +
  geom_hline(yintercept = mean(hsb2$write))
```

:::
::::

## Group Means

In the previous plot we were using the "grand mean" of `write`. If we use the two means instead...

:::: {.columns}
::: {.column width="30%"}

::: {.fragment fragment-index=1}

We see that the means of the two groups are different.

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

hsb2 %>% 
  group_by(gender) %>% 
   summarise(Group_means = mean(write))
```
:::

::: {.fragment fragment-index=2}
We can run a regression model that is the representation of the graph on the right
:::


:::
::: {.column width="70%"}

::: {.fragment fragment-index=1}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot code"


mean_female <- mean(hsb2$write[hsb2$gender == "female"])
mean_male <- mean(hsb2$write[hsb2$gender == "male"])

ggplot(hsb2, aes(x = gender, y = write)) +
  geom_point() +
  geom_hline(aes(yintercept = mean(mean_female), color = "Female"), 
             linetype = "dashed") +
  geom_hline(aes(yintercept = mean(mean_male), color = "Male"), 
             linetype = "dashed") +
    geom_hline(aes(yintercept = mean(hsb2$write), color = "Grand Mean")) +
  scale_color_manual(values = c("Female" = "blue", "Male" = "red", "Grand Mean" = "black")) +
  labs(color = "Means")
```

:::

:::
::::

## Coding Categorical Variables

We can code our categorical variables such that they are treated as continuous variables. Usually, we treat one group as "0", and the other group as "1". Let's say that `male` is $0$ and `female` is $1$:

:::: {.columns}
::: {.column width="60%"}


::: {.fragment fragment-index=1}
```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

hsb2$gender_binary <- ifelse(hsb2$gender == "female", 1, 0)

reg_gender_bin <- lm(write ~ gender_binary, data = hsb2)
summary(reg_gender_bin)
```
:::

:::
::: {.column width="40%"}


::: {.fragment fragment-index=2}

$\mathrm{write} = 50.12 + 4.87 \times \mathrm{gender}$

:::


::: {.fragment fragment-index=3}
<div style="font-size: 22px"> Remember that in regression, the intercept is *expected mean value* of $Y$ when $X = 0$, and the slope is the *expected mean change* in $Y$ per unit increase in $X$. </div>
:::

<ul style="font-size: 22px">  

::: {.fragment fragment-index=4}
<li> Then, since we coded $0$ to represent `male`, the intercept *is the mean of* the `male` group in `write` score.    </li>
:::

::: {.fragment fragment-index=5}
<li> And, since we coded $1$ to represent `female`, the slope *is the difference in means* between the `male` and `female` group in `write` score ($50.12 + 4.87 = 54.99$).    </li>
:::

</ul>


:::
::::


## Mean differences? Sounds familiar?

We just tested whether `male` and `female` are significantly different in mean `write` score. This is what you should know as an independent-samples *t*-test (**!**).

:::: {.columns}
::: {.column width="50%"}


::: {.fragment fragment-index=1}
```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

t.test(hsb2$write ~ hsb2$gender,
       var.equal = TRUE)

```
:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=2}
You should see that *t*-values, degrees of freedom, and *p*-values are the same as those for the slope on the previous slide. 
:::

::: {.fragment fragment-index=2}
::: {.callout-note}
## No Coincidences in Statistics

There are no coincidences in statistics. If two methods give some of the same results, they must be related in some way. When you see such patterns, ask yourself *why* two methods produce the same results. Once you answer that question, you will have gained tremendous insight.
:::
:::

:::
::::

::: {.fragment fragment-index=3}
*t*-tests don't exist really. For some reason everyone decides to teach *t*-tests separately from regression, but they are simply a specific case of a regression.
:::

## The Values "Do not Matter"

Since `gender` is not really a continuous variable, so we can turn it into *any* numeric values and still run a regression. For example:

:::: {.columns}
::: {.column width="60%"}

::: {.fragment fragment-index=1}
```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

hsb2$gender2 <- ifelse(hsb2$gender == "female", -314, -140)

reg_gender2 <- lm(write ~ gender2, data = hsb2)
summary(reg_gender2)
```
:::

:::
::: {.column width="40%"}


::: {.fragment fragment-index=2}
<div style="font-size: 22px"> The $R^2$ is the exact same as the regression from 2 slides ago; whether we code `male` and `female` as $0$ and $1$ or $-314$ and $-140$ (some random numbers I chose), *the models are equivalent*. However, the values of the intercept and slope are now meaningless. </div>
:::

::: {.fragment fragment-index=3}
<div style="font-size: 26px; padding-top: 14px; text-align: center;"> We can use this fact to our advantage! We can code our categorical variables in different ways to *test hypotheses* of mean differences between groups. </div>
:::

::: {.fragment fragment-index=4}
<div style="font-size: 18px; padding-top: 24px;"> **Note:** We will go over some popular coding schemes, but you may devise your own depending on your hypotheses. </div>
:::

:::
::::


## Dummy Coding

We have looked at the 2 groups case where we assigned $0$ to one group and $1$ to the other. However, what do we do when we have more than 2 groups? 

:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=1}

The `race` variable, for example, has four groups:

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

unique(hsb2$race)
```

:::


:::

::: {.column width="50%"}

::: {.fragment fragment-index=2}
<div style="font-size: 24px;"> We can't assign different numbers to each group (e.g., $0$, $1$, $2$, $3$) because then the variable will actually be treated as continuous. </div>
:::

:::
::::


::: {.fragment fragment-index=3}
<div style="font-size: 26px; padding-top: 14px; text-align: center;">  **Dummy coding** is a coding scheme to represent a categorical variable with more than 2 categories. </div>
:::

:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=4}
<div style="font-size: 24px; padding-top: 12px;"> In dummy coding, we use a series of $0$s and $1$s to define category membership. For example, we could say that: </div>

<ul style="font-size: 22px">  

<li> `c(1, 0, 0)`: African American </li>

<li> `c(0, 1, 0)`: Hispanic </li>

<li> `c(0, 0, 1)`: Asian </li>

<li> `c(0, 0, 0)`: White </li>

</ul>
:::

:::
::: {.column width="50%"}


::: {.fragment fragment-index=5}
<div style="font-size: 24px; padding-top: 12px;"> So, we will need *3 columns* in our data that define the category membership of each one of our participants. </div>

</br>

<div style="font-size: 24px"> This is the most straightforward coding scheme. Let's see what happens once we run a regression! </div>
:::

:::
::::


## Creating Dummy coded Columns

<span style="padding-bottom: 12px;"> The manual way of dummy coding variables in `R` involves creating separate columns. Here I show the `dummy_columns()` function from the `fastDummies` package. </span>

:::: {.columns}
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

dummy_dat <- dummy_columns(hsb2,
                          select_columns = "race",
                          remove_most_frequent_dummy = TRUE)

```     

</br>

<div style="font-size: 24px; padding-top: 12px;">  The `remove_most_frequent_dummy = TRUE` argument turns the most frequent category, `white` in our case, into the `c(0, 0, 0)` group. </div>


::: {.fragment fragment-index=1}
<div style="font-size: 24px; padding-top: 12px;">  With this coding scheme, the `c(0, 0, 0)` group is going to be the **reference group**. It will be clear why once we run the regression and look at the results. </div>
:::

:::
::: {.column width="50%"}


::: {.fragment fragment-index=1}
<div style="font-size: 24px">  This function returns the same data, but with the added dummy coded columns. Here are the columns compared to the original variable: </div>


```{r}
reactable::reactable(dummy_dat[,c("race", "race_african american", "race_asian", "race_hispanic")],
                     style = list(fontFamily = "Work Sans, sans-serif", fontSize = "1.105rem"),
                     pagination = FALSE, highlight = TRUE, height = 350)
```
:::

:::
::::

## Regression with Dummy coded Variables

We want to test group difference in `read` scores across the `race` variable. 

:::: {.columns}
::: {.column width="55%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

reg_race <- lm(read ~ `race_african american` + race_asian + race_hispanic, 
               dummy_dat)
summary(reg_race)
```

:::
::: {.column width="45%"}

 <div style="font-size: 16px; padding-bottom:12px;"> $\mathrm{read} = 53.92 - 7.12 \times \mathrm{African American} - 2.02 \times \mathrm{Asian} -7.26 \times \mathrm{Hispanic}$ </div>

::: {.fragment fragment-index=1}
<div style="font-size: 22px"> When all the dummy coded variables are $0$, we get the intercept, so: </div>
:::

<ul style="font-size: 22px">  

::: {.fragment fragment-index=1}
<li> $53.92 =$ the expected mean `read` score for `white` individuals. </li>
:::

::: {.fragment fragment-index=2}
<li> $-7.12 =$ the expected *difference* in mean `read` score for `african american` individuals compared to `white` individuals. </li>
:::

::: {.fragment fragment-index=3}
<li> $-2.02 =$ the expected *difference* in mean `read` score for `asian` individuals compared to `white` individuals. </li>
:::

::: {.fragment fragment-index=4}
<li> $-7.26 =$ the expected *difference* in mean `read` score for `hispanic` individuals compared to `white` individuals. </li>
:::

</ul>

::: {.fragment fragment-index=5}
<div style="font-size: 22px"> All the differences are with respect to the *reference group*, `white`. Further, the *p*-values of the slopes tell us whether the mean difference in `read` score between `white` and a specific group is significant! </div>
:::

:::
::::

## Dummy coding and Residuals

:::: {.columns}
::: {.column width="30%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

hsb2 %>% 
  group_by(race) %>% 
   summarise(`Group means` = mean(read))
```

<div style="font-size: 24px"> When calculating residuals, dummy coding tells the regression to *switch to using the mean of a specific group* when a member in that group is encountered. </div>

:::
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot code"


means <- hsb2 %>% 
  group_by(race) %>% 
   summarise(Group_means = mean(read)) %>% 
  as.data.frame

ggplot(hsb2, aes(x = race, y = read)) +
  geom_point() +
  geom_hline(aes(yintercept = means[1,2], color = "African American"), 
             linetype = "dashed") +
  geom_hline(aes(yintercept = means[2,2], color = "Asian"), 
             linetype = "dashed") +
  geom_hline(aes(yintercept = means[3,2], color = "Hispanic")) +
    geom_hline(aes(yintercept = means[4,2], color = "White")) +
  scale_color_manual(values = c("African American" = "blue", "Asian" = "red", "Hispanic" = "black",
                                "White" = "cyan"))  + 
  labs(color = "Means")
```

:::
::::


<div style="font-size: 22px"> So for `asian` individuals, the regression will use $51.9$ to calculate residuals, for  `white` individuals, the regression will use $53.9$, and so on... </div>
      


## Multiple Mean differences? Also Familiar

We just ran what you may know as one-way ANOVA. If you remember from intro stats, the calculation for the sum or squares error ($SSE$, the *residuals*) in ANOVAs involves this equation:


$$
\sum(X_{ij} - \bar{X}_j)^2,
$$

where $X_{ij}$ is the observation of the $i^{th}$ of individual in the $j^{th}$ group, and $\bar{X}_j$ is the mean of the $j^{th}$ group. The $\bar{X}_j$ part represents the mean that the residuals are calculated against changing for each group. 

:::: {.columns}
::: {.column width="50%"}


::: {.fragment fragment-index=1}
<div style="font-size: 24px; padding-bottom:18px;"> We can run a one-way ANOVA and see that we get the exact same $F$-value and degrees of freedom: </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

aov <- aov(read ~ factor(race), data = hsb2)
summary(aov)
```
:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=1}
<div style="font-size: 24px"> From the regression summary: </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

summary(reg_race)$fstatistic
```
:::

:::
::::


## Factor Variables in R

Coding variables manually takes some effort on the user's end. We can use `factor` variables to make the process smoother.


:::: {.columns}
::: {.column width="70%"}

::: {.fragment fragment-index=1}
we use the `factor()` function to turn any variable into a `factor`. Let's turn the `race` variable into a `factor`: 

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

hsb2$race <- factor(hsb2$race, 
                    levels = c("white", "african american", "asian", "hispanic"),
                    labels = c("W", "AA", "A", "H"))

str(hsb2$race)
```
:::
:::

::: {.column width="30%"}


::: {.fragment fragment-index=1}
<ul style="font-size: 22px">  

<li>  `levels = ` this argument tells the function the levels of our `factor`. `R` will choose levels automatically if you don't specify them. </li>

<li>  `labels = ` this argument renames each level respectively. It's optional; I use it to make labels shorter here. </li>

</ul>

:::

:::
::::


:::: {.columns}
::: {.column width="50%"}


::: {.fragment fragment-index=2}
<div style="font-size: 24px; padding-top: 12px;"> What's so good about `factor` variables? They do dummy coding for you behind the scenes! </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| output-location: column

# ouput on the right
contrasts(hsb2$race)
```
:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=2}
<div style="font-size: 24px; padding-top: 12px;"> If you run the `contrasts()` function on a dummy coded variable, `R` tells you how each variable is coded. The output is known as a *contrast matrix*.  </div>


<div style="font-size: 24px; padding-top: 12px;"> By default, the first level of the factor, `W` in our case, is coded as the reference group. </div>
:::

:::
::::


## Regression with Factor variable

:::: {.columns}
::: {.column width="55%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

reg_race_fct <- lm(read ~ race, hsb2)
summary(reg_race_fct)
```

:::
::: {.column width="45%"}

This is the exact regression output from some slides ago, but we did not have to create all the dummy coded columns manually 😀

::: {.fragment fragment-index=1}
If you are using `R`, I recommend using `factor` variables rather than manually creating dummy coded variables. 
:::


::: {.fragment fragment-index=1}
::: {.callout-note}
## Factor Variables are `R` specific

Every programming language has some common variable types (e.g., double, integer, character,...), but, as far as I can tell, `factor` variables are unique to `R`. The `R` programming language was created with statistics in mind. The fact that `factor` variables automatically dummy code categorical variables is a reflection of that.

:::
:::

:::
::::

## Other coding schemes

<div style="font-size: 24px;  padding-bottom:12px;"> So far we coded our `race` variable such that all the groups are compared to the `white` group. We can test different hypotheses if we change our coding scheme. Thanks to the way `factor` variables work, there is an elegant way of changing coding schemes!
 </div>


:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=1}

<div style="font-size: 24px"> The contrast matrix that defines the coding scheme is a property stored in every `factor` variable.</div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| output-location: column

contrasts(hsb2$race)
```
:::


:::
::: {.column width="50%"}


::: {.fragment fragment-index=2}

<div style="font-size: 24px;"> We can directly edit contrast matrix of a `factor` variable to change coding schemes. `R` also provides a series of functions that generate commonly used contrast matrices. For example: </div>


```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| output-location: column

# `4` stands for number of groups
contr.sum(4)
```
:::


:::
::::

:::: {.columns}
::: {.column width="70%"}

::: {.fragment fragment-index=3}
To edit the default contrast matrix inside a `factor` we use the assignment operator:

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| output-location: column

contrasts(hsb2$race) <- contr.sum(4)
# the contrast matrix has been updated
contrasts(hsb2$race)
```
:::

:::
::: {.column width="30%"}

::: {.fragment fragment-index=4}
<div style="font-size: 24px;padding-top: 18px;"> This type of coding scheme is known as *unweighted effects coding*. The unweighted part comes from the fact that the grand mean is calculated by assuming that every groups has the same sample size. </div>
:::

:::
::::


## Regression With Unweighted Effects Coding

:::: {.columns}
::: {.column width="55%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

reg_race_UEC <- lm(read ~ race, hsb2)
summary(reg_race_UEC)
```

:::

::: {.column width="45%"}

<div style="font-size: 24px"> The intercept, $49.82$, is the unweighted grand mean (it's the mean of the group means).</div>

::: {.fragment fragment-index=1}
<div style="font-size: 24px; padding-top:12px;"> Given the way we coded our `factor`, the slopes test whether either `W`, `AA`, or `A` individuals are significantly different in mean `read` score from the unweighted grand mean. </div>
:::

::: {.fragment fragment-index=2}
<div style="font-size: 24px; padding-top:12px;"> The only significantly different group is the `W` group, which was coded as `c(1, 0, 0)`. We can get the mean by plugging the values into the regression: </div>

<div style="font-size: 22px; padding-top:12px;">  $\bar{X}_{\mathrm{white}} = 49.825 +  4.1 \times 1 - 3.02 \times 0 + 2.08 \times 0 = 53.9$  </div>
:::

::: {.fragment fragment-index=3}
<div style="font-size: 18px; padding-top:12px;"> You can do the same for all the the other rows of the matrix on the previous slide and you will see that you will get the means of every group. </div>
:::

:::
::::

::: {.fragment fragment-index=4}
<div style="font-size: 24px;"> Also note that in this type of contrast, the row with `c(-1,-1,-1)`, `H` in our case, is always "left out", as we don't test any hypotheses about it directly. </div>
:::

## Weighted Effects Coding

The `contr.sum()` function creates contrasts assuming that all groups have the same number of participants. This is not true in out case: 

:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=1}
<div style="font-size: 24px"> This is our current contrast matrix for the `race` variable. </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

contrasts(hsb2$race)
```

:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=1}
<div style="font-size: 24px"> The groups definitely do not have the same sample size. </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

table(hsb2$race)
```
:::

:::
::::

::: {.fragment fragment-index=2}
<div style="font-size: 24px"> if $N_H$ is the sample size for the `H` group, then we achieve weighted effects coding by substituting the last row with $\mathrm{c}(-\frac{N_W}{N_H}, -\frac{N_{AA}}{N_H}, -\frac{N_A}{N_H})$ </div>
:::

:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=3}
<div style="font-size: 24px"> we can edit the last row of the contrast matrix of `race` directly: </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

contrasts(hsb2$race)[4,] <- c(-145/24, -20/24, - 11/24)
```
:::

:::
::: {.column width="50%"}


::: {.fragment fragment-index=3}
<div style="font-size: 24px"> The new contrast matrix of `race`: </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

contrasts(hsb2$race)
```
:::

:::
::::




## Regression With weighted Effects Coding

:::: {.columns}
::: {.column width="55%"}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

reg_race_WEC <- lm(read ~ race, hsb2)
summary(reg_race_WEC)
```

:::

::: {.column width="45%"}

<div style="font-size: 24px"> Now the intercept, $52.32$, which is the grand mean of `read` score! </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

mean(hsb2$read)
```


::: {.fragment fragment-index=1}
<div style="font-size: 24px"> Thus, the coefficients represent the the deviation of the `W`, `AA`, and `A` group in `read` score compared to the average `read` score for the whole sample. </div> 
:::

<ul style="font-size: 22px">  

::: {.fragment fragment-index=2}
<li> The `W` group is $1.69$ points *higher* in `read` score points compared to the sample average. (significant) </li>
:::

::: {.fragment fragment-index=3}
<li>  The `AA` group is $5.43$ points *lower* in `read` score points compared to the sample average. (significant) </li>
:::

::: {.fragment fragment-index=4}
<li>  The `A` group is $0.32$ points *lower* in `read` score points compared to the sample average. (not significant) </li>
:::

</ul>

:::
::::

::: {.fragment fragment-index=5}
<div style="font-size: 20px"> On the previous slide, we just changed the last row of the contrast matrix. You can also change the contrast matrix in its entirety if your hypotheses involve different contrasts. You need to know how to set up the contrasts matrix such that the coefficients test the hypotheses you want. see [Here](https://bookdown.org/pingapang9/linear_models_bookdown/contrasts.html){target="_blank"} a detailed explanation of contrasts. </div> 
:::



## Why all These Contrasts? 😕 A summary

<div style="font-size: 24px"> All of this contrast matrix talk may seem very confusing. You have the field of experimental psychology to thank for it 😅 I don't run experiments myself, so I don't think much about all of this. </div>


::: {.fragment fragment-index=1}
Jokes aside, contrasts are used to *test hypotheses*. You can test *any* type of mean comparison hypothesis by specifying the appropriate contrast.
:::

::: {.fragment fragment-index=2}
Why are we "allowed" to mess with regression coefficients in this way? As previously mentioned, categorical variables have no meaningful numerical scale (e.g., we don't say "pasta 🍝 is greater than pizza 🍕"). 
:::

::: {.fragment fragment-index=3}
However, we can assign some numerical scale to categorical variables such that the regression coefficients are "forced" to test the hypotheses we are interested in. 
:::

::: {.fragment fragment-index=4}
Do you *need* contrast matrices when testing hypotheses? Nah 🤷 In practice, you can just compare group means one by one. However, contrast matrices are an elegant way of testing multiple hypotheses at once. 
:::

::: {.fragment fragment-index=5}
<div style="font-size: 22px; padding-top: 20px;"> **Personal opinion:** Experimental psychologists really like their elegant designs, maybe a bit too much. It is my sense that contrast matrices are mostly a product of computational limitations of the past. These days, especially if you do Bayesian statistics, contrast do not have many uses (at least for mean comparisons). </div>
:::

## References 

<div id="refs"> </div>





















