---
title: "Lab 2: One-predictor Regression"
author: "Fabio Setti"
institute: "PSYC 7804 - Regression with Lab"
bibliography: Additional files/R packages.bib
csl: Additional files/apa.csl
title-slide-attributes:
  data-transition: "zoom"
  data-visibility: "uncounted"
format:
   revealjs:
      footer: "Lab 2: One-predictor Regression"
      width: 1280
      height: 720
      chalkboard: true
      slide-number: c/t 
      theme: Fabio_theme/Fabio_theme.scss
      navigation-mode: linear
      controls: false
      auto-stretch: false
      header-includes:
        - <script src="Fabio_theme/Fabio_theme.js"></script>

editor: source
---

## Font-awesome slide{visibility="hidden"}

uncounted and not visible slide to trigger the fontawesome extension

{{< fa thumbs-up >}} 


## Today's Packages and Data ü§ó

:::: {.columns}
::: {.column width="50%"}

```{r}
#| code-fold: true
#| eval: false
#| echo: true
#| code-line-numbers: false
#| code-summary: "Install Packages Code"
#| classes: code-150

install.packages("rio")
install.packages("psych")
install.packages("tidyverse")
install.packages("car")
```


```{r}
#| eval: true
#| echo: true
#| code-line-numbers: false
#| warning: false
#| classes: code-150

library(car)
library(psych)
library(tidyverse)
theme_set(theme_classic(base_size = 16, 
                        base_family = 'serif'))
```

</br>

<div style="font-size: 26px">

::: {.panel-tabset}

### `car`

The `car` package [@Fox_etal_2024] contains many helper functions to analyze and explore regression results. It was originally created to be used along a regression book written by the same authors.

### `tidyverse`

The `tidyverse` package [@Wickham_RStudio_2023] loads a suite of packages that help with data cleaning and visualization. Among others, `tidyverse` loads both `dplyr` and `ggplot2`.

### `psych`

The `psych` package [@Revelle_2024a] includes MANY functions that help with data exploration and running analyses typically done in the social sciences.

:::
</div>

:::
::: {.column width="50%"}


<center> **Data** </center>

</br>

```{r}
#| warning: false
#| classes: code-125
#| echo: true
#| code-line-numbers: false

GPA <- rio::import("https://github.com/quinix45/PSYC-7804-Regression-Lab-Slides/raw/refs/heads/main/Slides%20Files/Data/GPA.sav")

str(GPA, vec.len = 2)
```



:::
::::


## Describe and Plot your data

Whenever working with new data it's good practice to run descriptive statistics and visualizations

:::: {.columns}
::: {.column width="30%"}


```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false


# run in your R to get clearer output
describe(GPA)
```


<br>

::: {.fragment fragment-index=1}

The two variables seem fairly normally distributed. 

:::

:::
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: true 
#| code-fold: true
#| code-summary: "Plot code"
#| code-line-numbers: false

GPA %>% 
# When creating multiple plots at once, long data works best
pivot_longer(cols = colnames(GPA),
             names_to = "variable",
             values_to = "value") %>% 
          ggplot(aes(x  = value)) +
          geom_density() +
# the facet_wrap() fucntion generates plots for each value in "variable"
          facet_wrap(~variable, scales = "free") +
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.line.y =  element_blank())

```

:::
::::



## Scatterplot {auto-animate="true"}

A scatterplot is ideal for visualizing bivariate relations 

:::: {.columns}
::: {.column width="30%"}

We have seen this code before

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false

ggplot(GPA,
      aes(x = act, y = year1gpa)) +
      geom_point()
```


:::
::: {.column width="70%"}


```{r}
#| eval: true
#| echo: false 
#| code-line-numbers: false

ggplot(GPA,
      aes(x = act, y = year1gpa)) +
      geom_point()
```

:::
::::

::: {.fragment fragment-index=1}

There seems to be a positive trend (as ACT scores increase, first year GPA also increases). Let's draw a **regression line** to confirm that.  

:::

## Scatterplot {auto-animate="true" data-visibility="uncounted"}

A scatterplot is ideal for visualizing bivariate relations 

:::: {.columns}
::: {.column width="30%"}


```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false

ggplot(GPA,
      aes(x = act, y = year1gpa)) +
      geom_point() +
      geom_smooth(method = "lm", 
                  se = FALSE)
```


:::
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: false 
#| code-line-numbers: false

ggplot(GPA,
      aes(x = act, y = year1gpa)) +
      geom_point() +
      geom_smooth(method = "lm", se = FALSE)
```

:::
::::

The **regression line** confirms our observation. 


## Scatterplot {auto-animate="true" data-visibility="uncounted"}

A scatterplot is ideal for visualizing bivariate relations 

:::: {.columns}
::: {.column width="30%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false

ggplot(GPA,
      aes(x = act, y = year1gpa)) +
      geom_point() +
      geom_smooth(method = "lm", 
                  se = FALSE) +
      geom_smooth(method = "loess",
                  color = "red",
                  se = FALSE)
```


:::
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: false 
#| code-line-numbers: false

ggplot(GPA,
      aes(x = act, y = year1gpa)) +
      geom_point() +
      geom_smooth(method = "lm", se = FALSE) +
      geom_smooth(method = "loess", 
                  color = "red",
                  se = FALSE)
```

:::
::::

::: {.fragment fragment-index=1}

<div style="font-size: 22px"> We can add a loess line, which usually helps with checking for **non-linear trends**. However, here it should probably not be trusted much. Why? Because there are too few points (only 20), and the loess line is influenced by just a few observations.  
  </div>

:::

## Correlation 

Correlation, usually denoted with $r$, measures the strength of linear association between two variables (always ranges between $-1$ and $1$). 


:::: {.columns}
::: {.column width="50%"}


<center>




**The Math**

::: {.fragment fragment-index=1}

$r_{xy} = \frac{ \sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y}) }{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}}$

:::

</center>

::: {.fragment fragment-index=2}
Although this formula looks a bit scary, we can break it down in familiar elements:
:::

<ul style="font-size: 26px" >


::: {.fragment fragment-index=3}
<li> $\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})$ is the covariance (equivalent to correlation) between $x$ and $y$ </li> 
:::

::: {.fragment fragment-index=4}
<li> $\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}$ is equivalent to the variance of $x$ </li> 

<li> $\sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}$ is equivalent to the variance of $y$ </li> 
:::
::: {.fragment fragment-index=5}
<div style="font-size: 22px"> all the denominator does is guarantee that $r$ is always between $-1$ and $1$. It *standardizes* the covariance. </div>
:::

</ul>

:::
::: {.column width="50%"}

<center>

**The code**

</center>

::: {.fragment fragment-index=6}

<div style="font-size: 26px">
Calculating correlations by hand is very simple in R
</div>

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

cov_xy <- sum((GPA$year1gpa - mean(GPA$year1gpa)) * (GPA$act - mean(GPA$act)))
var_x <- sqrt(sum((GPA$year1gpa - mean(GPA$year1gpa))^2))
var_y <- sqrt(sum((GPA$act - mean(GPA$act))^2))

cov_xy/(var_x * var_y)
```

:::

::: {.fragment fragment-index=7}

<div style="font-size: 26px">
In practice you would just use the `cor()` function to calculate the correlation between two variables.
</div>

```{r}
#| eval: true
#| echo: true 
#| classes: code-150

cor(GPA$year1gpa, GPA$act)
```

:::

:::
::::


## Significance Test for Correlations

The `cor()` function does not run any significance test for our correlations. To get significance tests for correlations, you can use the `corr_test()` function from the `psych` package

:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=1}

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

# Note that the line below is equivalent to corr.test(GPA$year1gpa, GPA$act)
psych::corr.test(GPA)
```
:::

::: {.fragment fragment-index=2}

The output of `corr_test()` can be ever so slightly confusing ü§® 
:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=3}

<div style="font-size: 26px">
To get more insight into what `R` functions actually do, sometimes you want to save results as objects and explore the object content.
</div>

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

correlation <- corr.test(GPA)

# this prints the name of all the elements of the `correlation` object
names(correlation)
```
:::

::: {.fragment fragment-index=4}

<div style="font-size: 26px">
Turns out the `corr_test()` function stores much more information than it lets on. This is the case for most `R` functions, which will save a lot of information inside a `list` object.
</div>

:::
:::
::::


## `corr_test()` output

:::: {.columns}
::: {.column width="40%"}


Keep an eye out for the matching output between the table below and the code on the right.

<br>


```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

corr.test(GPA)
```


:::
::: {.column width="60%"}

<ul style="font-size: 26px">

::: {.fragment fragment-index=1}
<li> The `r` element is the correlation matrix between our 2 variables </li>

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

correlation$r
```
:::

::: {.fragment fragment-index=2}

<li> The `p` element is the *p*-value for every element of the correlation matrix </li>

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

correlation$p
```
:::


::: {.fragment fragment-index=3}

<li> The `ci` element is the confidence interval (which is not printed by default) </li>

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

correlation$ci
```
:::

</ul>
:::
::::

## Running a Regreassion in R


To run regressions in R we use the `lm()` function (which stands for linear model). In the case of our data, we want to see whether ACT scores (`act`) predict GPA in the first year of college (`year1gpa`)


:::: {.columns}
::: {.column width="60%"}

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

reg <- lm(year1gpa ~ act, data = GPA)
summary(reg)

```
:::
::: {.column width="40%"}

`lm(Y ~ X, data = your data)` 

<ul style="font-size: 26px">


<li> `Y`: the name of your dependent variable (DV) </li>

<li> `X`: the name of your independent variable (IV) </li>

<li>  `data = your data`: The data that contains your variables </li>

</ul>

<div style="font-size: 22px">  
**NOTE**: if we were to just run `lm(year1gpa ~ act)`, we would get an error. Neither  `year1gpa` and `act` are objects in our `R` environment. We need to tell `R` where to find `year1gpa` and `act` by adding `data = GPA`. 
</div>


:::
::::


## Interpreting Regression Output

:::: {.columns}
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

reg <- lm(year1gpa ~ act, data = GPA)
summary(reg)

```
:::
::: {.column width="50%"}

$$Y = b_0 + b_1X + e$$

<ul style="font-size: 26px">

::: {.fragment fragment-index=1}

<li> $Y$ = The DV (`year1gpa`) </li>

:::

::: {.fragment fragment-index=2}

<li> $b_0 = 0.46$, the predicted value of $Y$ when $X = 0$, AKA the **intercept** </li>

:::

::: {.fragment fragment-index=3}

<li>  $b_1 = 0.09$, the predicted change in $Y$ per each 1-unit increase in $X$, AKA the **slope** </li>

:::

::: {.fragment fragment-index=4}

<li>  $X$ =  the IV (`act`) </li>

:::


::: {.fragment fragment-index=5}
<li>  $\epsilon = 0.61$, the expected spread of the residuals $e$ around the regression line, AKA the **residual variance**\* </li>
:::

</ul>

::: {.fragment fragment-index=5}
<div style="font-size: 22px"> 
\***NOTE**: if you just do applied research, you will probably never have to think about $\epsilon$. However, there is great insight gained into understanding why it's there (refer to appendix)

</div>
:::

:::
::::


## Making Predictions

For many reasons, once we have a regression model, we may want to predict values of $Y$ based on some values of $X$. 


:::: {.columns}
::: {.column width="30%"}

::: {.fragment fragment-index=1}
<div style="font-size: 24px"> From the previous slide, we found that $\hat{Y} = .46 + .09\times X$ (we don't look at $e$ when making predictions!) So, to find values of $\hat{Y}$ we plug in some values of $X$. </div>
:::

<ul style="font-size: 22px; padding-top: 26px;">  


::: {.fragment fragment-index=2}
<li> If $X = 23$, then $\hat{Y} = .46 + .09\times23 = 2.53$     </li>

<li> If $X = 27$, then $\hat{Y} = .46 + .09\times27 = 2.89$   </li>
:::

</ul>

::: {.fragment fragment-index=3}

<center> **Predictions Represent the *mean* expected value of *Y* given some value of *X*. Predictions are *always* on the regression line** </center>
:::


:::
::: {.column width="70%"}

::: {.fragment fragment-index=2}
```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| code-fold: true
#| code-summary: "Plot Code"
#| classes: code-125

ggplot(GPA,
       aes(x = act, y = year1gpa)) +
  geom_point(alpha = .5, shape = 1) +
  geom_smooth(method = "lm",
              se = FALSE) +
  geom_segment(x = 23,
               xend = 23,
               y = 0,
               yend = coef(reg)[1] + coef(reg)[2]*23,
               linetype = 2) +
    geom_segment(x = 0,
                 xend = 23,
                 y = coef(reg)[1] + coef(reg)[2]*23,
                 yend = coef(reg)[1] + coef(reg)[2]*23,
                 linetype = 2)  +
  geom_segment(x = 27,
               xend = 27,
               y = 0,
               yend = coef(reg)[1] + coef(reg)[2]*27,
               linetype = 2) +
    geom_segment(x = 0,
                 xend = 27,
                 y = coef(reg)[1] + coef(reg)[2]*27,
                 yend = coef(reg)[1] + coef(reg)[2]*27,
                 linetype = 2)   
  
```
:::

:::
::::


## Making Predictions The "right way"

Whenever you are doing calculations in R, you *should never* copy and paste output numbers because there may be rounding error (and your code will not be generalizable!). 

:::: {.columns}
::: {.column width="40%"}

::: {.fragment fragment-index=1}
<div style="font-size: 26px"> For any `lm()` object, the `coef()` function will extract the exact coefficient values (intercepts and slopes in order of predictor):</div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

coef(reg)
```
:::

::: {.fragment fragment-index=2}
<div style="font-size: 26px; padding-top: 18px;"> So, the appropriate way of calculating the predicted value of $Y$ if $X = 23$ is: </div>

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

as.numeric(coef(reg)[1] + coef(reg)[2]*23)
```
:::

:::
::: {.column width="60%"}

::: {.fragment fragment-index=3}
<div style="font-size: 26px">  Usually, you need predictions for some new data, so you would use the `predict()` function. The predict function requires that you input data that has *the exact same structure as the original data*, but without the $Y$ variable. </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

# first we create new data that we want to get predictions on
pred_dat <- data.frame("act" = seq(20, 30, by = .01))
```
:::

::: {.fragment fragment-index=4}
<div style="font-size: 26px; padding-top: 18px;"> Here I save the predictions as a new variable. Look at the `pred_dat` object after running the code below: </div>

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

pred_dat$y_pred <- predict(reg, pred_dat)
str(pred_dat)
```
:::

:::
::::

## Predictions and residuals

`lm()` objects also save predictions for all data points ($\hat{Y}_i$) and the residuals for all data points ($e_i$).

:::: {.columns}
::: {.column width="50%"}
**Predictions**

::: {.fragment fragment-index=1}

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

predictions <- fitted(reg)
str(predictions, give.attr = FALSE)
```
These are the predicted values of $Y$ given the observed $X$ values. 

:::

:::
::: {.column width="50%"}

**Residuals**


::: {.fragment fragment-index=2}
```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

residuals <- residuals(reg)
str(residuals, give.attr = FALSE)
```

<div style="font-size: 24px">  These are the differences between the *observed* values of $Y$ and the *predicted* values of $Y$, so $e_i = Y_i - \hat{Y}_i$. </div>
:::

::: {.fragment fragment-index=3}
The residuals always sum to 0: 

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false


sum(residuals)
```
:::

:::
::::

::: {.fragment fragment-index=4}
::: {.callout-note}
## Dimensions of Objects
You should notice that both `predictions` and  `residuals` are vectors of length 20. This may seem obvious (if it is not, give it some thought), but checking the **dimensions** of objects is always a good way of making sure that you are getting what you expected from an R function. 
:::
:::

## Visualizing Predictions and Residuals {auto-animate="true"}

:::: {.columns}
::: {.column width="30%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

ggplot(GPA,
 aes(x = act, y = year1gpa)) +
 geom_point(shape = 1) 
```

<ul style="font-size: 26px">

<li> The *black circles* are the observed data points.</li>

</ul>



:::
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false

ggplot(GPA,
       aes(x = act, y = year1gpa)) +
    geom_point(shape = 1) 

```

:::
::::


## Visualizing Predictions and Residuals {auto-animate="true" data-visibility="uncounted"}

:::: {.columns}
::: {.column width="30%"}


```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

ggplot(GPA,
 aes(x = act, y = year1gpa)) +
 geom_point(shape = 1) +
 geom_point(aes(y = predictions), 
                colour = "red")
```

<ul style="font-size: 26px">

<li> The *black circles* are the observed data points.</li>

<li> The *red dots* are the predicted values for our data points. </li>

</ul>

:::

::: {.column width="70%"}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false


ggplot(GPA,
       aes(x = act, y = year1gpa)) +
    geom_point(shape = 1) +
   geom_point(aes(y = predictions), 
              colour = "red")

```

:::
::::

## Visualizing Predictions and Residuals {auto-animate="true" data-visibility="uncounted"}

:::: {.columns}
::: {.column width="30%"}


```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-115

ggplot(GPA,
 aes(x = act, y = year1gpa)) +
 geom_point(shape = 1) +
 geom_point(aes(y = predictions), 
                color = "red") +
 geom_segment(y = predictions,
              x = GPA$act,
              xend = GPA$act,
              yend = GPA$year1gpa,
              linetype = 2,
              alpha = .5)
   
```

<ul style="font-size: 24px">

<li> The *black circles* are the observed data points.</li>

<li> The *red dots* are the predicted values for our data points. </li>

<li> The *length of the dashed lines* are the residuals, which represent the difference between observed values (black circles) and predicted values (red dots) </li>

</ul>

:::

::: {.column width="70%"}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false

ggplot(GPA,
       aes(x = act, y = year1gpa)) +
   geom_point(shape = 1) +
   geom_point(aes(y = predictions), 
              colour = "red") +
   geom_segment(y = predictions,
                x = GPA$act,
                xend = GPA$act,
                yend = GPA$year1gpa,
                linetype = 2,
                alpha = .5)
   

```

:::
::::

## Normality of Residuals

The fact that residuals must be normally distributed is an assumption built into regression (why? The appendix explains it). QQplots can help us infer how much residuals deviate from normality.

:::: {.columns}
::: {.column width="30%"}

::: {.fragment fragment-index=1}

The `qqPlot()` function from the `car` package is helpful here:

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

car::qqPlot(reg, id = FALSE)
```

:::

::: {.fragment fragment-index=2}

As long as roughly 95% of the residuals (dots) are within the band, there should be no cause for concern.

<div style="font-size: 22px"> **NOTE:** since the shaded area represents a 95% confidence interval, seeing roughly 5% of the residuals outside would be within our expectations. </div>

:::





:::
::: {.column width="70%"}

::: {.fragment fragment-index=1}

```{r}
#| eval: true
#| echo: false 
#| code-line-numbers: false
#| classes: code-125

car::qqPlot(reg, id = FALSE)
```

:::

:::
::::

## Plotting Residuals Against Predictors

Another way of checking that residuals are evenly distributed around 0, and do not show any type of relationship with the predictor. 

:::: {.columns}
::: {.column width="30%"}

::: {.fragment fragment-index=1}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

ggplot(GPA, 
  aes(x = act, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(method = "loess",
              se = FALSE)
```

Once again, the loess line does not do too well because there are few points. The points look reasonably distributed around the 0 line to me.

:::

:::
::: {.column width="70%"}

::: {.fragment fragment-index=1}

```{r}
#| eval: true
#| echo: false 
#| code-line-numbers: false

ggplot(GPA, 
  aes(x = act, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  geom_smooth(method = "loess",
              se = FALSE) 
```

:::

:::
::::

## Standardized Regression Output

<div style="font-size: 26px">  To run standardized regression in R we first standardize our data. The `scale()` function standardizes (or mean-centers) data. The `scale()` function can be a bit particular ([see here](https://www.r-bloggers.com/2021/12/how-to-use-the-scale-function-in-r/){target="_blank"} if you have troubles with it) </div>


:::: {.columns}
::: {.column width="50%"}

```{r}
#| eval: true
#| echo: true 
#| classes: code-125
#| code-line-numbers: false

GPA_std <- data.frame(scale(GPA))

reg_std <- lm(year1gpa ~ act, data = GPA_std)
summary(reg_std)
```
:::

::: {.column width="50%"}

::: {.fragment fragment-index=1}

The meaning of the results does not change, just the coefficients. So...

$$Y = 0 + .36X,$$

$.36$ is also the correlation coefficient between `act` and `year1gpa`. Just remember that X and Y are in standard deviation units.

:::


::: {.fragment fragment-index=2}
<div style="font-size: 24px"> 
üßê Standardizing our variables simply changes measurement units. You can think of it as switching between degrees Fahrenheit and Celsius. the state of the world (temperature) remains the same, only the numbers that we use to describe it change üßò
</div>
:::

:::
::::




## References 

<div id="refs"> </div>


# Appendix: The "true" Regression Model and its assumptions {data-visibility="uncounted"}


## The "true" Regression Model

You may have seen regression models formulated in different ways. Turns out, the notation can have subtle differences that change the meaning of what you are looking at:


:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=1}

<ul style="font-size: 26px">  

$Y_i = b_0 + b_1X_i + e_i$ 


$\hat{Y_i} = b_0 + b_1X_i$ 


$\hat{Y} = b_0 + b_1X$ 

Where

<li> $Y_i$: the *observed* $Y$ value of participant $i$ </li>


<li> $\hat{Y_i}$: the *predicted* $Y$ value of participant $i$ </li>


<li> $\hat{Y}$: the *predicted* $Y$ value (e.g., predictions on data not yet observed) </li>


</ul>

:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=2}

Now, the "real" regression model is actually

$Y \sim N(\mu = b_0 + b_1X, \sigma = \epsilon)$

<div style="font-size: 16px"> in math, "$\sim$" reads "distributed as"</div>

:::

::: {.fragment fragment-index=3}
Equivalently, you may see this written as

$Y = b_0 + b_1X + e$ where, $e \sim N(\mu = 0, \sigma = \epsilon)$
:::

::: {.fragment fragment-index=4}
<div style="font-size: 24px">  The second formulation is more explicit about the main assumption of regression, that the residuals, $e$, are normally distributed. Let's explore a bit more...</div>

</br>

<div style="font-size: 22px"> **NOTE**: regression makes no assumption about the distribution of the predictors (i.e., $X$ need not to be normally distributed, regression does not care) </div>
:::


:::
::::


## The Data Generating Process

On the previous slide you saw we had $Y_i$, $\hat{Y_i}$, $\hat{Y}$, and crucially, just $Y$ (). One important thing to keep in mind is that, whenever we run statistics, the question that we are asking is really...


::: {.fragment fragment-index=1}
<div style="font-size: 34px"> <center> **What is the probabilistic process by which the data that I observed came into existence? (i.e., the data generating process)** </center> </div>
:::


:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=2}
Likewise, regression tries to answer how your observed sample of the $Y$ random variable comes into existence. 
:::

<br>

::: {.fragment fragment-index=4}
The $Y \sim N(b_0 + b_1X, \epsilon)$ (for simplicity, I dropped the $\mu$ and $\sigma$) is how linear regression thinks $Y$ crosses over the dotted line and ends up in our data. 
:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=3}
![](Images/data_process.png){width="76%"}
:::

:::
::::

## The Meaning of A regression model


:::: {.columns}
::: {.column width="50%"}

The regression output was:

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

summary(reg)
```


:::
::: {.column width="50%"}

if $Y \sim N(b_0 + b_1X, \epsilon)$ is our model...

::: {.fragment fragment-index=1}

...what we are saying with the results on the left is that $Y$ is normally distributed with 

- **mean**  of $0.46 +  0.08X_{ACT}$, the $b_0 + b_1X$ part, and 

- **standard deviation** of $0.61$, the $\epsilon$ (residual variance) part.
:::


::: {.fragment fragment-index=2}
Let's look a bit more into what the *mean* and *standard deviation* represent conceptually!
:::

:::
::::

## Predictable and Unpredictable Parts

You think of the normal distribution as a predictable and unpredictable part. The predictable part is the mean, whereas the unpredictable part is the standard deviation. 

:::: {.columns}
::: {.column width="50%"}

::: {.fragment fragment-index=1}

For example, if I generate data from a normal distribution with 0 standard deviation

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

# generate 5 observations from N~(2, 0)
rnorm(5, mean = 2, sd = 0)
```

The outcome is fully predictable, and we always get 2. 

:::

:::
::: {.column width="50%"}

::: {.fragment fragment-index=2}

```{r}
#| eval: true
#| echo: true 
#| code-line-numbers: false
#| classes: code-125


set.seed(2356)
# add randomness to observations on the left
rnorm(5, mean = 2, sd = 0) + 
rnorm(5, mean = 0, sd = 4) 

# the two lines above are equivalent to just
# set.seed(2356)
# rnorm(5, mean = 2, sd = 4) 
```

In contrast, here we know that our values will be centered around 2, the mean. However, there is unpredictable randomness introduced by the standard deviation of 4. 
:::

:::
::::

::: {.fragment fragment-index=3}
In regression, we swap the mean of the normal distribution for the **regression line** (predictable part), and the randomness for $\epsilon$, the **residual variance** (unpredictable part). 
:::

## A Visual Representation

We can visualize what our $Y$ variable should look like according to our regression model in a *perfect world*. 

:::: {.columns}
::: {.column width="30%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125

ggplot(GPA, 
  aes(x = act, y = year1gpa)) +  
  # assume we have no data yet
  geom_point(alpha = 0) +
      geom_smooth(method = "lm", se = FALSE) +
  ylim(0, 5)
```

The line is what the *mean* of `year1gpa` should be at every value of `act`. However, there is no data yet.
:::
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false

ggplot(GPA, 
  aes(x = act, y = year1gpa)) +  
  geom_point(alpha = 0) +
      geom_smooth(method = "lm", se = FALSE)  +
  ylim(0, 5)
```

:::
::::

## A Visual Representation


We can visualize what our $Y$ variable should look like according to our regression model in a *perfect world*. 

:::: {.columns}
::: {.column width="30%"}

```{r}
#| eval: false
#| echo: true 
#| code-line-numbers: false
#| classes: code-125
#| code-fold: true
#| code-summary: "Plot Code"

# Generate idealized data

set.seed(7757)

# generate X values
act <- rep(seq(min(GPA$act), max(GPA$act), by =.1), 150)

# the code below generates data according to the regression model

intercept <- coef(reg)[1]
slope <- coef(reg)[2]
residual_var <- sigma(reg)

ideal_data <- rnorm(length(act), mean = intercept + act*slope, sd = sigma(reg))

# plot the idealized data

ggplot(mapping =  aes(x = act, y = ideal_data)) +  
  geom_point(alpha = .2)+
      geom_smooth(method = "lm", se = FALSE) +
  ylim(0, 5)
```

Each "column of dots" is normally distributed around the line, with standard deviation of $.61$, the residual variance ($\epsilon$). The dots are essentially our residuals, $e$. 


::: {.fragment fragment-index=1}
<center>
***This is the data our regression model expects to see***
</center>
:::

:::
::: {.column width="70%"}

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false


# Generate idealized data

set.seed(7757)

# generate X values
act <- rep(seq(min(GPA$act), max(GPA$act), by =.1), 150)

# the code below generates data according to the regression model

intercept <- coef(reg)[1]
slope <- coef(reg)[2]
residual_var <- sigma(reg)

ideal_data <- rnorm(length(act), mean = intercept + act*slope, sd = sigma(reg))

# plot the idealized data

ggplot(mapping =  aes(x = act, y = ideal_data)) +  
  geom_point(alpha = .2)+
      geom_smooth(method = "lm", se = FALSE) +
  ylim(0, 5)
```

:::
::::

::: {.fragment fragment-index=2}
The $Y$ variable looking like this is the only "assumption" of regression.
:::

## Statistical Models VS Reality

:::: {.columns}
::: {.column width="50%"}

The $Y$ variable never looks like this.

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false

ggplot(mapping =  aes(x = act, y = ideal_data)) +  
  geom_point(alpha = .2)+
      geom_smooth(method = "lm", se = FALSE) +
  ylim(0, 5)
```

:::
::: {.column width="50%"}

This is what the $Y$ variable looks like usually (our data).

```{r}
#| eval: true
#| echo: false
#| code-line-numbers: false

ggplot(GPA, 
  aes(x = act, y = year1gpa)) +  
  geom_point() +
      geom_smooth(method = "lm", se = FALSE) +
  ylim(0, 5)
```

:::
::::

::: {.fragment fragment-index=1}
<div style="font-size: 24px"> Violating assumptions? Assumptions are $always$ "violated". To "check assumptions" is knowing what data your model expects to see and evaluating how different the observed data is (QQplots for example, help you do this). </div>
:::

::: {.fragment fragment-index=2}
<div style="font-size: 24px">  
Whether you think your observed data it "too different" from the expected data is usually a *very subjective* process. 
</div>
:::




  